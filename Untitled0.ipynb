{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6SV63m0qO5_T","outputId":"0f878ace-304d-46a5-dfe5-5a31c583a94c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting scenedetect\n","  Downloading scenedetect-0.6.2-py3-none-any.whl (117 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/117.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from scenedetect) (8.1.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scenedetect) (1.25.2)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from scenedetect) (4.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scenedetect) (4.66.2)\n","Installing collected packages: scenedetect\n","Successfully installed scenedetect-0.6.2\n","Collecting datasets\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow\u003e=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill\u003c0.3.9,\u003e=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]\u003c=2024.2.0,\u003e=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub\u003e=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.3.1)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (23.2.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.4.1)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (6.0.5)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (1.9.4)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-\u003edatasets) (4.0.3)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003e=0.19.4-\u003edatasets) (4.10.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.6)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003edatasets) (2023.4)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.1-\u003epandas-\u003edatasets) (1.16.0)\n","Installing collected packages: dill, multiprocess, datasets\n","Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n","Collecting POT\n","  Downloading POT-0.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.16 in /usr/local/lib/python3.10/dist-packages (from POT) (1.25.2)\n","Requirement already satisfied: scipy\u003e=1.6 in /usr/local/lib/python3.10/dist-packages (from POT) (1.11.4)\n"]}],"source":["!pip install scenedetect\n","!pip install datasets\n","!pip install POT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtr_UG28OHe8"},"outputs":[],"source":["\"\"\" utility functions\"\"\"\n","import re\n","import os\n","from os.path import basename\n","\n","import gensim\n","import torch\n","from torch import nn\n","import json\n","from statistics import median\n","\n","PAD = 0\n","UNK = 1\n","START = 2\n","END = 3\n","PAD_TOKEN = '\u003cpad\u003e'\n","UNK_TOKEN = '\u003cunk\u003e'\n","START_TOKEN = '\u003cstart\u003e'\n","END_TOKEN = '\u003cend\u003e'\n","\n","import torch\n","import numpy as np\n","from torch.autograd import Variable\n","from collections import defaultdict, Counter, OrderedDict\n","from os.path import join\n","from itertools import chain\n","\n","\n","class OrderedCounter(Counter, OrderedDict):\n","    \"\"\"Counter that remembers the order elements are first encountered\"\"\"\n","    def __repr__(self):\n","        return '%s(%r)' % (self.__class__.__name__, OrderedDict(self))\n","\n","    def __reduce__(self):\n","        return self.__class__, (OrderedDict(self),)\n","\n","\n","def to_var(x):\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return x\n","\n","\n","def idx2word(idx, i2w, pad_idx):\n","    sent_str = [str()]*len(idx)\n","    for i, sent in enumerate(idx):\n","        for word_id in sent:\n","            if word_id == pad_idx:\n","                break\n","            sent_str[i] += i2w[str(word_id.item())] + \" \"\n","        sent_str[i] = sent_str[i].strip()\n","    return sent_str\n","\n","\n","def interpolate(start, end, steps):\n","\n","    interpolation = np.zeros((start.shape[0], steps + 2))\n","\n","    for dim, (s, e) in enumerate(zip(start, end)):\n","        interpolation[dim] = np.linspace(s, e, steps+2)\n","\n","    return interpolation.T\n","\n","\n","def expierment_name(args, ts):\n","    exp_name = str()\n","    exp_name += \"BS=%i_\" % args.batch_size\n","    exp_name += \"LR={}_\".format(args.learning_rate)\n","    exp_name += \"EB=%i_\" % args.embedding_size\n","    exp_name += \"%s_\" % args.rnn_type.upper()\n","    exp_name += \"HS=%i_\" % args.hidden_size\n","    exp_name += \"L=%i_\" % args.num_layers\n","    exp_name += \"BI=%i_\" % args.bidirectional\n","    exp_name += \"LS=%i_\" % args.latent_size\n","    exp_name += \"WD={}_\".format(args.word_dropout)\n","    exp_name += \"ANN=%s_\" % args.anneal_function.upper()\n","    exp_name += \"K={}_\".format(args.k)\n","    exp_name += \"X0=%i_\" % args.x0\n","    exp_name += \"TS=%s\" % ts\n","\n","    return exp_name\n","\n","def count_data(path):\n","    \"\"\" count number of data in the given path\"\"\"\n","    matcher = re.compile(r'[0-9]+\\.json')\n","    match = lambda name: bool(matcher.match(name))\n","    names = os.listdir(path)\n","    n_data = len(list(filter(match, names)))\n","    return n_data\n","\n","def make_vocab(wc, vocab_size):\n","    word2id, id2word = {}, {}\n","    word2id[PAD_TOKEN] = PAD\n","    word2id[UNK_TOKEN] = UNK\n","    word2id[START_TOKEN] = START\n","    word2id[END_TOKEN] = END\n","    for i, (w, _) in enumerate(wc.most_common(vocab_size), 4):\n","        word2id[w] = i\n","    return word2id\n","\n","def convert_word2id(w, word2id):\n","    try:\n","        wid = word2id[w]\n","        if wid \u003c 30000:\n","            return wid\n","        return UNK\n","    except:\n","        return UNK\n","\n","def make_embedding(id2word, w2v_file, initializer=None):\n","    attrs = basename(w2v_file).split('.')  #word2vec.{dim}d.{vsize}k.bin\n","    w2v = gensim.models.Word2Vec.load(w2v_file).wv\n","    vocab_size = len(id2word)\n","    emb_dim = int(attrs[-3][:-1])\n","    embedding = nn.Embedding(vocab_size, emb_dim).weight\n","    if initializer is not None:\n","        initializer(embedding)\n","\n","    oovs = []\n","    with torch.no_grad():\n","        for i in range(len(id2word)):\n","            # NOTE: id2word can be list or dict\n","            if i == START:\n","                embedding[i, :] = torch.Tensor(w2v['\u003cs\u003e'])\n","            elif i == END:\n","                embedding[i, :] = torch.Tensor(w2v[r'\u003c\\s\u003e'])\n","            elif id2word[i] in w2v:\n","                embedding[i, :] = torch.Tensor(w2v[id2word[i]])\n","            else:\n","                oovs.append(i)\n","    return embedding, oovs\n","\n","def count_data_stat(path):\n","    \"\"\" count statistics of the data\"\"\"\n","    max_article_split = []\n","    max_sentence_split = []\n","    median_article_split = []\n","    median_sentence_split = []\n","    for split in ['train', 'val', 'test']:\n","        art_sents = []\n","        data_path = join(path, split)\n","        for i in range(count_data(data_path)):\n","            with open(join(data_path, '{}.json'.format(i))) as f:\n","                js = json.loads(f.read())\n","                art_sents.append(js['article'])\n","        article_size = [len(story) for story in art_sents] #number of sentences in an article\n","        sentence_size = [len(row) for row in chain.from_iterable([story for story in art_sents])] #max number of words in a sentence\n","        max_article_size = max(article_size)\n","        max_sentence_size = max(sentence_size)\n","        median_article_size = median(article_size)\n","        median_sentence_size = median(sentence_size)\n","        max_article_split.append(max_article_size)\n","        max_sentence_split.append(max_sentence_size)\n","        median_article_split.append(median_article_size)\n","        median_sentence_split.append(median_sentence_size)\n","        print('######## Statistics for', split,'split: ######')\n","        print('Number of data:', count_data(data_path))\n","        print('Max number of sentences in an article:', max_article_size)\n","        print('Median number of sentences in an article:', median_article_size)\n","        print('Max number of words in a sentence:', max_sentence_size)\n","        print('Median number of words in a sentence:', median_sentence_size)\n","\n","    return max(max_article_split), max(max_sentence_split), max(median_article_split), max(median_sentence_split)\n","\n","#MAX_ARTICLE_SIZE, MAX_SENTENCE_SIZE, MEAN_ARTICLE_SIZE, MEAN_SENTENCE_SIZE = count_data_stat(DATA_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH1b4c_1OLzv"},"outputs":[],"source":["from transformers import GPT2Tokenizer as GPT2Tok\n","from transformers import BertTokenizer as BertTok\n","#import sentencepiece as spm\n","import nltk\n","\n","class Capita:\n","    def forward(self, text):\n","        # words = nltk.tokenize.word_tokenize(text)\n","        words = text.split(\" \")\n","        final_words = []\n","        for word in words:\n","            if not word.isalpha():\n","                final_words.append(word.lower())\n","            else:\n","                if word.islower():\n","                    pass\n","                elif word.isupper():\n","                    final_words.append(\"⇧\")\n","                elif word[0].isupper() and word[1:].islower():\n","                    final_words.append(\"↑\")\n","                else:\n","                    final_words.append(\"↑\")\n","                final_words.append(word.lower())\n","        return \" \".join(final_words)\n","\n","    def backward(self, text):\n","        words = text.split(\" \")\n","        final_words = []\n","        all_caps = False; capitalized = False\n","        for w in words:\n","            if w == \"⇧\": all_caps = True\n","            elif w == \"↑\": capitalized = True\n","            else:\n","                final_word = w\n","                if all_caps: final_word = final_word.upper()\n","                elif capitalized:\n","                    if len(final_word) \u003c= 1: final_word = final_word.upper()\n","                    else: final_word = final_word[0].upper()+final_word[1:]\n","                final_words.append(final_word)\n","                all_caps = False; capitalized = False\n","        return \" \".join(final_words)\n","\"\"\"\n","class BPETokenizer:\n","    def __init__(self, bpe_model, use_capita=True):\n","        self.sp = spm.SentencePieceProcessor()\n","        self.sp.Load(bpe_model)\n","        self.use_capita = use_capita\n","\n","        self.pad_tok, self.start_tok, self.end_tok = \"\u003cpad\u003e\", \"\u003cstart\u003e\", \"\u003cend\u003e\"\n","        self.pad_id, self.start_id, self.end_id = tuple(self.sp.piece_to_id(p) for p in [self.pad_tok, self.start_tok, self.end_tok])\n","\n","        self.vocab_size = self.sp.get_piece_size()\n","\n","        if self.use_capita:\n","            self.cpt = Capita()\n","\n","    def tokenize(self, text):\n","        if len(text) == 0:\n","            return []\n","        if text[:len(self.start_tok)] == self.start_tok and text[len(self.start_tok)] != \" \":\n","            text = text.replace(self.start_tok, self.start_tok+\" \")\n","\n","        if self.use_capita:\n","            text = self.cpt.forward(text)\n","        tokens = self.sp.encode_as_pieces(text)\n","        tokens = [w for i, w in enumerate(tokens) if (i \u003c (len(tokens)-1) and tokens[i+1] not in [\"⇧\", \"↑\"]) or i==(len(tokens)-1)]\n","        if tokens[0] == \"▁\":\n","            tokens = tokens[1:]\n","        return tokens\n","\n","    def encode(self, text):\n","        tokens = self.tokenize(text)\n","        token_ids = [self.sp.piece_to_id(w) for w in tokens]\n","        return token_ids\n","\n","    def decode(self, token_ids):\n","        text = self.sp.decode_ids(token_ids).replace(\"⇧\", \" ⇧\").replace(\"↑\", \" ↑\")\n","        if self.use_capita:\n","            text = self.cpt.backward(text)\n","        text = text.replace(self.start_tok+\" \", self.start_tok)\n","        return text\n","\"\"\"\n","class BERTCacheTokenizer:\n","    def __init__(self):\n","        self.cache = {}\n","        self.cache_keys = []\n","        self.tokenizer = BertTok.from_pretrained(\"bert-base-uncased\")\n","        # self.tokenizer.max_len = 10000 # This was removed in later transformer tokenizers\n","\n","    def encode(self, text):\n","        if text in self.cache:\n","            return self.cache[text]\n","\n","        output = self.tokenizer.encode(text)\n","\n","        if len(self.cache) \u003e 1000:\n","            del self.cache[self.cache_keys.pop(0)]\n","        self.cache[text] = output\n","        self.cache_keys.append(text)\n","        return output\n","\n","class GPT2Tokenizer:\n","    def __init__(self):\n","        self.tokenizer = GPT2Tok.from_pretrained(\"gpt2\")\n","        # self.tokenizer.max_len = 10000\n","\n","        self.pad_tok, self.start_tok, self.end_tok = \"\u003cPAD\u003e\", \" ST\", \" END\"\n","\n","        self.pad_id = 0\n","        self.start_id = self.tokenizer.encode(self.start_tok)[0]\n","        self.end_id =   self.tokenizer.encode(self.end_tok)[0]\n","        self.vocab_size =  self.tokenizer.vocab_size\n","\n","    def tokenize(self, text):\n","        return self.tokenizer.tokenize(text)\n","\n","    def encode(self, text):\n","        return self.tokenizer.encode(text)\n","\n","    def decode(self, token_ids):\n","        return self.tokenizer.decode(token_ids)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m95_FSYPOzuo"},"outputs":[],"source":["import torch.utils.data\n","from os.path import join, exists\n","import re, json, cv2, os, sys, glob\n","import xml.etree.ElementTree as ET\n","#import torchvision\n","import random, itertools\n","#from torchvision import transforms as t\n","#from torchvision import transforms\n","from PIL import Image\n","#import torchvision.models as models\n","import numpy as np\n","\n","  # Standard PySceneDetect imports:\n","from scenedetect.video_manager import VideoManager\n","from scenedetect.scene_manager import SceneManager\n","\n","# For content-aware scene detection:\n","from scenedetect.detectors.content_detector import ContentDetector\n","\n","\n","\n","class MultimodalDataset(torch.utils.data.dataset.Dataset):\n","    def __init__(self, split: str, path: str):\n","        # print(path)\n","        self._data_path = join(path, split)\n","        self._n_data = _count_data(self._data_path)\n","\n","    def __len__(self) -\u003e int:\n","        return self._n_data\n","\n","    def __getitem__(self, i: str):\n","        #print(\"js path\", join(self._data_path, '{}.json'.format(i)))\n","        #print(\"i\", i)\n","\n","        with open(join(self._data_path, '{}.json'.format(i))) as f:\n","            js = json.loads(f.read())\n","\n","        original_frames = []\n","        vidcap = cv2.VideoCapture(join(self._data_path, '{}.mp4'.format(i)))\n","        success,image = vidcap.read()\n","        count = 0\n","        while success:\n","            #if count % 120:\n","            if count % 480:\n","                #image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n","                #original_frames.append(cv2.resize(image, (640, 360)))\n","                original_frames.append(torch.tensor(cv2.resize(image, (640, 360))))\n","            success, image = vidcap.read()\n","            count += 1\n","            if count \u003e 100: #reach cuda limit\n","                break\n","        # Stack it into a tensor\n","        if original_frames:\n","            video = torch.stack(original_frames, 0)\n","        else:\n","            video = torch.tensor([])\n","\n","\n","        thumbnail = cv2.imread(join(self._data_path, '{}.png'.format(i)))\n","\n","        try:\n","            transcript = ET.parse(join(self._data_path, '{} (a.en).xml'.format(i))).getroot()\n","        except:\n","            transcript = ''\n","\n","\n","        return js, thumbnail, transcript, video\n","\n","class EXMSMODataset(MultimodalDataset):\n","    \"\"\" single article sentence -\u003e single abstract sentence\n","    (dataset created by greedily matching ROUGE)\n","    \"\"\"\n","\n","    def __init__(self, split, DATA_DIR):\n","        super().__init__(split, DATA_DIR)\n","        files = glob.glob(join(self._data_path, \"*.json\"))\n","        self.file_id = [os.path.split(x)[1].replace('.json', '') for x in files]\n","\n","    def __getitem__(self, i):\n","        js, thumbnail, transcript_xml, video = super().__getitem__(self.file_id[i])\n","\n","        transcript = []\n","        if not transcript_xml == '':\n","            for w in transcript_xml:\n","                transcript.append(w.text)\n","            #print(\"transcript_xml\", transcript_xml)\n","            transcripts = '; '.join(transcript).replace('\u0026#39;', '\\'')\n","        else:\n","            transcripts = ''\n","\n","        title = js['title']\n","        description= js['description']\n","\n","        return  self.file_id[i], description, video, title, thumbnail, transcripts\n","\n","\n","\n","class MultimodalNoTruncateDataset(torch.utils.data.dataset.Dataset):\n","    def __init__(self, split: str, path: str):\n","        self._data_path = join(path, split)\n","        self._n_data = _count_data(self._data_path)\n","\n","    def __len__(self) -\u003e int:\n","        return self._n_data\n","\n","    def __getitem__(self, i: str):\n","        #print(\"js path\", join(self._data_path, '{}.json'.format(i)))\n","        #print(\"i\", i)\n","\n","        with open(join(self._data_path, '{}.json'.format(i))) as f:\n","            js = json.loads(f.read())\n","\n","        original_frames = []\n","        vidcap = cv2.VideoCapture(join(self._data_path, '{}.mp4'.format(i)))\n","        vframe = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        vfps    = vidcap.get(cv2.CAP_PROP_FPS)\n","        vs    = vframe/vfps\n","\n","\n","        thumbnail = cv2.imread(join(self._data_path, '{}.png'.format(i)))\n","\n","        transcript = ET.parse(join(self._data_path, '{} (a.en).xml'.format(i))).getroot()\n","\n","\n","        return js, thumbnail, transcript, vframe, vfps, vs\n","\n","\n","class EXMSMONoTruncateDataset(MultimodalNoTruncateDataset):\n","    \"\"\" single article sentence -\u003e single abstract sentence\n","    (dataset created by greedily matching ROUGE)\n","    \"\"\"\n","\n","    def __init__(self, split, DATA_DIR):\n","        super().__init__(split, DATA_DIR)\n","        files = glob.glob(join(self._data_path, \"*.json\"))\n","        self.file_id = [os.path.split(x)[1].replace('.json', '') for x in files]\n","\n","    def __getitem__(self, i):\n","        js, thumbnail, transcript_xml, vframe, vfps, vs = super().__getitem__(self.file_id[i])\n","\n","        transcript = []\n","        for w in transcript_xml:\n","            transcript.append(w.text)\n","        #print(\"transcript_xml\", transcript_xml)\n","        transcripts = '; '.join(transcript).replace('\u0026#39;', '\\'')\n","\n","        title = js['title']\n","        description= js['description']\n","\n","        return  self.file_id[i], description, vframe, title, vfps, vs, thumbnail, transcripts\n","\n","\n","\n","class MultimodalWithSceneDataset(torch.utils.data.dataset.Dataset):\n","    def __init__(self, split: str, path: str):\n","        self._data_path = join(path, split)\n","        self._n_data = _count_data(self._data_path)\n","\n","    def __len__(self) -\u003e int:\n","        return self._n_data\n","\n","    def __getitem__(self, i: str):\n","        #print(\"js path\", join(self._data_path, '{}.json'.format(i)))\n","        #print(\"i\", i)\n","\n","        with open(join(self._data_path, '{}.json'.format(i))) as f:\n","            js = json.loads(f.read())\n","\n","        original_frames = []\n","        vidcap = cv2.VideoCapture(join(self._data_path, '{}.mp4'.format(i)))\n","        success,image = vidcap.read()\n","        count = 0\n","        while success:\n","            #if count % 120:\n","            if count % 360:\n","                #image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n","                #original_frames.append(cv2.resize(image, (640, 360)))\n","                original_frames.append(torch.tensor(cv2.resize(image, (640, 360))))\n","            success, image = vidcap.read()\n","            count += 1\n","            if count \u003e 100: #reach cuda limit\n","                break\n","        # Stack it into a tensor\n","        video = torch.stack(original_frames, 0)\n","\n","\n","        scene_list = find_scenes(join(self._data_path, '{}.mp4'.format(i)))\n","        thumbnail = cv2.imread(join(self._data_path, '{}.png'.format(i)))\n","\n","        #transcript = ET.parse(join(self._data_path, '{} (a.en).xml'.format(i))).getroot()\n","\n","        #return js, thumbnail, transcript, video, scene_list\n","        return js, thumbnail, video, scene_list\n","\n","class EXMSMOWithSceneDataset(MultimodalWithSceneDataset):\n","    \"\"\" single article sentence -\u003e single abstract sentence\n","    (dataset created by greedily matching ROUGE)\n","    \"\"\"\n","\n","    def __init__(self, split, DATA_DIR):\n","        super().__init__(split, DATA_DIR)\n","        files = glob.glob(join(self._data_path, \"*.json\"))\n","        self.file_id = [os.path.split(x)[1].replace('.json', '') for x in files]\n","\n","    def __getitem__(self, i):\n","        js, thumbnail, video, scene_list = super().__getitem__(self.file_id[i])\n","\n","        #transcript = []\n","        #for w in transcript_xml:\n","        #    transcript.append(w.text)\n","        #print(\"transcript_xml\", transcript_xml)\n","        #transcripts = '; '.join(transcript).replace('\u0026#39;', '\\'')\n","\n","        title = js['title']\n","        description= js['description']\n","\n","        #return  self.file_id[i], description, video, title, thumbnail, transcripts, scene_list\n","        return  self.file_id[i], description, video, title, thumbnail, scene_list\n","\n","class MSMO(MultimodalDataset):\n","    def __init__(self, split, DATA_DIR):\n","        super().__init__(split, DATA_DIR)\n","        files = glob.glob(join(self._data_path + 'article', \"*.txt\"))\n","        self.file_id = [os.path.split(x)[1].replace('.txt', '') for x in files]\n","\n","    def __getitem__(self, i):\n","\n","        article_path = join(self._data_path + 'article' , '{}.txt'.format(i))\n","        document, extreme_summaries = get_art_abs(article_path)\n","\n","        original_frames = []\n","        vidcap = cv2.VideoCapture(join(self._data_path, '{}.mp4'.format(i)))\n","        vframe = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        vfps    = vidcap.get(cv2.CAP_PROP_FPS)\n","        vs    = vframe/vfps\n","\n","\n","        thumbnail = cv2.imread(join(self._data_path, '{}.png'.format(i)))\n","\n","        transcript = ET.parse(join(self._data_path, '{} (a.en).xml'.format(i))).getroot()\n","\n","\n","\n","        js, thumbnail, video, scene_list = super().__getitem__(self.file_id[i])\n","\n","        #transcript = []\n","        #for w in transcript_xml:\n","        #    transcript.append(w.text)\n","        #print(\"transcript_xml\", transcript_xml)\n","        #transcripts = '; '.join(transcript).replace('\u0026#39;', '\\'')\n","\n","        title = js['title']\n","        description= js['description']\n","\n","        #return  self.file_id[i], description, video, title, thumbnail, transcripts, scene_list\n","        return  self.file_id[i], description, video, title, thumbnail, scene_list\n","\n","\n","def read_story_file(text_file):\n","    with open(text_file, \"r\") as f:\n","        # sentences are separated by 2 newlines\n","        # single newlines might be image captions\n","        # so will be incomplete sentence\n","        lines = f.read().split('\\n\\n')\n","    return lines\n","\n","def fix_missing_period(line):\n","    \"\"\"Adds a period to a line that is missing a period\"\"\"\n","    if \"@summary\" in line:\n","        return line\n","    if line == \"\":\n","        return line\n","    return line + \" .\"\n","\n","def get_art_abs(story_file):\n","    \"\"\" return as list of sentences\"\"\"\n","    lines = read_story_file(story_file)\n","\n","    # Lowercase, truncated trailing spaces, and normalize spaces\n","    lines = [' '.join(line.lower().strip().split()) for line in lines]\n","\n","    # Put periods on the ends of lines that are missing them (this is a problem\n","    # in the dataset because many image captions don't end in periods;\n","    # consequently they end up in the body of the article as run-on sentences)\n","    lines = [fix_missing_period(line) for line in lines]\n","\n","    # Separate out article and abstract sentences\n","    article_lines = []\n","    highlights = []\n","    next_is_highlight = False\n","    next_is_body = False\n","    for idx, line in enumerate(lines):\n","        #print(\"line\", line)\n","        if line == \"\":\n","            #print(\"empty\")\n","            continue # empty line\n","        elif line.startswith(\"@body\"):\n","            #print(\"line.startswith(body)\")\n","            next_is_body = True\n","            article_lines.append(line.replace(\"@body\", '') + '.')\n","        elif line.startswith(\"@summary\"):\n","            #print(\"line.startswith(summary)\")\n","            next_is_highlight = True\n","            next_is_body = False\n","            highlights.append(line.replace(\"@summary\", '') + '.')\n","        elif next_is_body:\n","            #print(\"next_is_body\")\n","            article_lines.append(line)\n","        elif next_is_highlight:\n","            #print(\"next_is_highlight\")\n","            highlights.append(line)\n","\n","    return ' '.join(article_lines), ' '.join(highlights)\n","\n","def find_scenes(video_path, threshold=80.0):\n","    # Create our video \u0026 scene managers, then add the detector.\n","    video_manager = VideoManager([video_path])\n","    scene_manager = SceneManager()\n","    scene_manager.add_detector(ContentDetector(threshold=threshold))\n","    # Improve processing speed by downscaling before processing.\n","    video_manager.set_downscale_factor()\n","    # Start the video manager and perform the scene detection.\n","    video_manager.start()\n","    scene_manager.detect_scenes(frame_source=video_manager, frame_skip=360)\n","    # Each returned scene is a tuple of the (start, end) timecode.\n","    scene_list = scene_manager.get_scene_list()\n","\n","    scene_frame_list = []\n","    for i, scene in enumerate(scene_list):\n","        #print(\n","        #    'Scene %2d: Start %s / Frame %d, End %s / Frame %d' % (\n","        #    i+1,\n","        #    scene[0].get_timecode(), scene[0].get_frames(),\n","        #    scene[1].get_timecode(), scene[1].get_frames(),))\n","        scene_frame_list.append(int(scene[1].get_frames()/360))\n","        #scene_frame_list.append(scene[1].get_frames())\n","\n","    return torch.Tensor(scene_frame_list)\n","\n","\n","\n","\n","def _count_data(path):\n","    \"\"\" count number of data in the given path\"\"\"\n","    files = glob.glob(join(path, \"*.json\"))\n","    n_data = len(files)\n","    return n_data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IKDSXGwSy5cL"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import numpy as np\n","import re, math\n","import pickle as pkl\n","from torch.nn import functional as F\n","#from transformers import BertTokenizer, BertModel\n","import os\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","import torch.nn.utils.rnn as rnn_utils\n","import time\n","# from utils import PAD, UNK, END, START, UNK_TOKEN, PAD_TOKEN\n","from torch.nn.functional import softplus\n","import torchvision.models as models\n","import nltk\n","from torchvision import transforms\n","from PIL import Image\n","from nltk import word_tokenize\n","from transformers import CLIPFeatureExtractor, CLIPModel, CLIPTokenizer, CLIPTextModel\n","from nltk.tokenize import sent_tokenize\n","from transformers.pipelines import pipeline\n","from nltk.tag import pos_tag\n","import cv2\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)\n","\n","class CLIPSum(nn.Module):\n","    def __init__(self, text_hidden_size, video_hidden_size,\n","                max_summary_word, max_summary_pic):\n","\n","        super(CLIPSum, self).__init__()\n","        self.pos = ['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']\n","\n","        self.face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","\n","\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.max_summary_word = max_summary_word\n","        self.max_summary_pic = max_summary_pic\n","\n","        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.featureextractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        #self.vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        #self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        #self.text_model.to(device)\n","        #self.vision_model.eval()\n","        #self.vision_model.to(device)\n","\n","\n","        self.cliptext = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        #self.cliptext.eval()\n","        #self.clipvision = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.clip.eval()\n","        self.cliptext.eval()\n","        #self.clipvision.eval()\n","        self.hidden_size = 512\n","        self.topic_size = 100\n","        self.dropout = 0.1\n","        #self.image_hidden_size=768\n","        self.num_attention_head = 16\n","        #self.wordattn = nn.MultiheadAttention(embed_dim=self.hidden_size, num_heads=self.num_attention_head, dropout=self.dropout)\n","        #self.word_layer_norm = nn.LayerNorm(self.hidden_size)\n","        #self.wordlstm= nn.LSTM(self.hidden_size*2, self.hidden_size, num_layers=2, bidirectional=False, batch_first = True)\n","\n","        self.word_pret_pos_encoder = PositionalEncoding(self.hidden_size)\n","        #word_encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=16)\n","        #self.word_pret_transformer_model = nn.TransformerEncoder(word_encoder_layer, num_layers=12)\n","        self.word_pret_transformer_model = nn.Transformer(self.hidden_size, nhead=self.num_attention_head, num_encoder_layers=12, batch_first=True)\n","\n","        self.word_modalspecific_transformer_model = nn.Transformer(self.hidden_size, nhead=self.num_attention_head, num_encoder_layers=12, batch_first=True)\n","\n","        #self.word_pos_encoder = PositionalEncoding(self.hidden_size + 1)\n","        self.word_transformer_model = nn.Transformer(self.hidden_size, nhead=self.num_attention_head, num_encoder_layers=12, batch_first=True)\n","\n","        self.image_pret_pos_encoder = PositionalEncoding(self.hidden_size)\n","        #image_encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=16)\n","        #self.image_pret_transformer_model = nn.TransformerEncoder(image_encoder_layer, num_layers=12)\n","        self.image_pret_transformer_model = nn.Transformer(self.hidden_size, nhead=self.num_attention_head, num_encoder_layers=12, batch_first=True)\n","        self.image_modalspecific_transformer_model = nn.Transformer(self.hidden_size, nhead=self.num_attention_head, num_encoder_layers=12, batch_first=True)\n","\n","        #self.image_pos_encoder = PositionalEncoding(self.hidden_size + 1)\n","        self.image_transformer_model = nn.Transformer(self.hidden_size, nhead=self.num_attention_head, num_encoder_layers=12, batch_first=True)\n","\n","\n","        self.v2tattn = nn.MultiheadAttention(embed_dim=self.hidden_size, num_heads=self.num_attention_head, batch_first=True, dropout=self.dropout)\n","        self.v2tattn_layer_norm = nn.LayerNorm(self.hidden_size)\n","        self.v2tattn_linear = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.v2tattn_linear_layer_norm = nn.LayerNorm(self.hidden_size )\n","\n","        self.t2vattn = nn.MultiheadAttention(embed_dim=self.hidden_size, num_heads=self.num_attention_head, batch_first=True, dropout=self.dropout)\n","        self.t2vattn_layer_norm = nn.LayerNorm(self.hidden_size)\n","        self.t2vattnn_linear = nn.Linear(self.hidden_size , self.hidden_size )\n","        self.t2vattn_linear_layer_norm = nn.LayerNorm(self.hidden_size )\n","\n","        self.coattn = nn.Linear((self.hidden_size )*2, self.hidden_size)\n","        #self.image_layer_norm = nn.LayerNorm(self.hidden_size)\n","        #self.imagelstm= nn.LSTM(self.hidden_size*2, self.hidden_size, num_layers=2, bidirectional=False, batch_first = True)\n","\n","\n","        with open(\"kmeans_model_100.pkl\", \"rb\") as f:\n","            self.k_means =  pkl.load(f)\n","\n","        self.outputs2vocab = nn.Linear(self.hidden_size, 1)\n","\n","       # self.pic_linear_z = nn.Linear(in_features=self.hidden_size*2, out_features=self.hidden_size)\n","\n","        self.outputs2coverframe = nn.Linear(self.hidden_size, 1)\n","\n","\n","    def forward(self, input_text, input_video, text_summary_length=11):\n","\n","        batch_size = len(input_text)\n","\n","        batch_sent = []\n","        batch_sent_num = []\n","        batch_sent_pad = []\n","        text_token_list = []\n","        text_id_list = []\n","        pos_list = []\n","        for text in input_text:\n","            text = text.replace(\"Please subscribe HERE http://bit.ly/1rbfUog\", \"\")\n","            text = text.replace(\"#BBCNews\", \"\")\n","            text = text.replace(\"\\n\", \" \\n \")\n","            sents = sent_tokenize(text)\n","            text_id_l = []\n","            text_token_l = []\n","            pos_l = []\n","            for sent in sents:\n","                text_t = word_tokenize(sent)\n","                pos_parse = nltk.pos_tag(text_t)\n","                pos_t = []\n","                for p in pos_parse:\n","                    print(\"p\", p)\n","                    try:\n","                        pos_t.append(self.pos.index(p[1])+1)\n","                    except:\n","                        pos_t.append(0)\n","                for t in text_t:\n","                    text_token = self.tokenizer(t , return_tensors = \"pt\", padding=True, truncation=True, max_length=50)\n","                    text_id_l.append(int(text_token['input_ids'][0][1]))\n","                text_token_l.extend(text_t)\n","                pos_l.extend(pos_t)\n","            text_token_list.append(text_token_l)\n","            print(\"text_id_l\", text_id_l)\n","            pos_list.append(torch.LongTensor(pos_l[:77]))\n","            text_id_list.append(torch.LongTensor(text_id_l[:77])) #clip max length\n","\n","            batch_sent.append(sents)\n","            batch_sent_num.append(len(sents))\n","\n","        for batch in batch_sent:\n","            batch += [' '] * (max(batch_sent_num) - len(batch))\n","            batch_sent_pad.append(batch)\n","\n","        batch_sent_pad_t = [list(x) for x in zip(*batch_sent_pad)]\n","        sent_feature = []\n","        word_feature = []\n","        pad_sent_len = []\n","        sent_feature = []\n","        last_len = 0\n","        #for sent in batch_sent_pad_t:\n","        #print(\"sent\", sent)\n","        pos_id_list = torch.nn.utils.rnn.pad_sequence(pos_list, batch_first=True).cuda()\n","        text_id_list = torch.nn.utils.rnn.pad_sequence(text_id_list, batch_first=True).cuda()\n","        print(\"text_id_list\", text_id_list.size())\n","        print(\"pos_id_list\", pos_id_list.size())\n","        word_feature = self.cliptext(text_id_list).last_hidden_state\n","        word_feature = self.word_pret_pos_encoder(word_feature)\n","        #word_feature = self.word_pret_transformer_model(word_feature.permute(1,0,2)).permute(1,0,2)\n","\n","\n","        print(\"word_feature\", word_feature.size())\n","        #text_overall_feature = self.clip.get_text_features(text_token['input_ids'].cuda())\n","        #text_token_list.append(text_token_l)\n","        sent_feature = self.clip.get_text_features(text_id_list)\n","        print(\"sent_feature\", sent_feature.size())\n","\n","        text_b,text_s,text_d = word_feature.size()\n","\n","        pos_id_target = torch.zeros(text_b, text_s, self.hidden_size).cuda()\n","        pos_id_target[:, :, :1] = pos_id_list.unsqueeze(2)\n","\n","        #word_feature = torch.cat([word_feature, pos_id_list], dim=2)\n","\n","        word_feature = self.word_modalspecific_transformer_model(word_feature, pos_id_target)\n","\n","\n","\n","\n","        topic_distance_t = torch.from_numpy(self.k_means.transform(sent_feature.detach().cpu().numpy())).cuda()\n","\n","        topic_distance_t_target = torch.zeros(text_b, self.hidden_size).cuda()\n","        topic_distance_t_target[:, :self.topic_size] = topic_distance_t\n","\n","        word_feature = self.word_pret_transformer_model(word_feature, topic_distance_t_target.unsqueeze(1).expand(-1,text_s , -1))\n","\n","        last_len = 0\n","\n","        if batch_size \u003e 1:\n","            input_video = torch.nn.utils.rnn.pad_sequence(input_video, batch_first=True)\n","\n","        scene_frame_pad_batch_list = input_video[:, :50,:,:,:] #max for cuda\n","\n","        v_image_features = []\n","        v_num_face = []\n","        for i, images in enumerate(scene_frame_pad_batch_list.permute(1,0,4,2,3)):\n","            num_faces = []\n","            image_batch = []\n","            for batch in images:\n","                img = transforms.ToPILImage()(batch.squeeze_(0))\n","                #print(\"img\", img.size())\n","                image_batch.append(img)\n","                p#rint(\"img\", img.size())\n","                faces = self.face_cascade.detectMultiScale(cv2.cvtColor(np.array(img), cv2.COLOR_BGR2GRAY), 1.1, 4)\n","                num_faces.append(len(faces))\n","            v_num_face.append(torch.LongTensor(num_faces))\n","            image_token = self.featureextractor(image_batch, return_tensors = \"pt\")\n","            v_image_features.append(self.clip.get_image_features(image_token['pixel_values'].cuda()))\n","\n","        v_num_face = torch.stack(v_num_face, dim=1)\n","        print(\"v_num_face\", v_num_face.size())\n","        print(\"v_num_face\", v_num_face)\n","        image_feature = torch.stack(v_image_features, dim=1)\n","        image_feature = self.image_pret_pos_encoder(image_feature)\n","\n","\n","        video_feature = torch.mean(image_feature, dim=1)\n","\n","        print(\"video_feature\", video_feature.size())\n","        #image_feature = torch.cat([image_feature, v_num_face.unsqueeze(2).cuda()], dim=2)\n","        print(\"image_feature\", image_feature.size())\n","        video_b,video_s,video_d = image_feature.size()\n","\n","        face_target = torch.zeros(video_b, video_s, self.hidden_size).cuda()\n","        face_target[:, :, :1] = v_num_face.unsqueeze(2)\n","\n","        image_feature = self.image_modalspecific_transformer_model(image_feature, face_target.cuda())\n","\n","\n","\n","\n","        print(\"video_d\", video_d)\n","        topic_distance_v = torch.from_numpy(self.k_means.transform(video_feature.detach().cpu().numpy())).cuda()\n","\n","        topic_distance_v_target = torch.zeros(video_b, self.hidden_size).cuda()\n","        topic_distance_v_target[:, :self.topic_size] = topic_distance_v\n","\n","\n","        image_feature = self.image_pret_transformer_model(image_feature, topic_distance_v_target.unsqueeze(1).expand(-1,video_s , -1))\n","\n","\n","        v2t_attn, _ = self.v2tattn(image_feature, word_feature, word_feature)\n","        v2t_attn = self.v2tattn_layer_norm(v2t_attn) + image_feature\n","        v2t_attn_linear = self.v2tattn_linear(v2t_attn.reshape(-1, video_d))\n","        v2t_attn = self.v2tattn_linear_layer_norm(v2t_attn_linear.view(video_b,video_s,video_d)) + v2t_attn\n","\n","        t2v_attn, _ = self.t2vattn(word_feature, image_feature, image_feature)\n","        t2v_attn = self.v2tattn_layer_norm(t2v_attn) + word_feature\n","        t2v_attn_linear = self.v2tattn_linear(t2v_attn.reshape(-1, text_d))\n","        t2v_attn = self.v2tattn_linear_layer_norm(t2v_attn_linear.view(text_b,text_s,text_d)) + t2v_attn\n","\n","        print(\"v2t_attn\", v2t_attn.size())\n","        print(\"t2v_attn\", t2v_attn.size())\n","        #print(\"torch.cat([v2t_attn.squeeze(1), t2v_attn.squeeze(1)],dim=1)\", torch.cat([v2t_attn.squeeze(1), t2v_attn.squeeze(1)],dim=1).size())\n","        overall_feature = self.coattn(torch.cat([torch.mean(v2t_attn, dim=1), torch.mean(t2v_attn, dim=1)],dim=1))\n","\n","\n","        #overall_feature = torch.mean(torch.stack([video_feature, sent_feature],dim=1), dim=1)\n","\n","\n","        topic_distance = torch.from_numpy(self.k_means.transform(overall_feature.detach().cpu().numpy())).cuda()\n","\n","        topic_distance_target = torch.zeros(text_b, self.hidden_size).cuda()\n","        topic_distance_target[:, :self.topic_size] = topic_distance\n","\n","\n","        #word_feature_topic = torch.cat((word_feature, topic_distance.unsqueeze(1).expand(-1,text_s , -1)), dim=2)\n","        #word_attn_output, _ = self.wordattn(word_feature.permute(1, 0, 2), topic_distance_target.unsqueeze(1).expand(-1,text_s , -1).permute(1, 0, 2), topic_distance_target.unsqueeze(1).expand(-1,text_s , -1).permute(1, 0, 2))\n","        #word_attn_output = self.word_layer_norm(word_attn_output)\n","        #print(\"word_attn_output\", word_attn_output.size())\n","        #word_attn_output = word_attn_output.permute(1, 0, 2)\n","        #word_feature = self.word_pos_encoder(word_feature)\n","        word_attn_output = self.word_transformer_model(word_feature, topic_distance_target.unsqueeze(1).expand(-1,text_s , -1))\n","        #word_lstm_out = torch.cat([word_feature, word_attn_output.permute(1, 0, 2)], -1)\n","        #print(\"word_lstm_out\", word_lstm_out.size())\n","        #word_lstm_out1, (_, _) = self.wordlstm(word_lstm_out)\n","        #print(\"word_lstm_out1\", word_lstm_out1.size())\n","        print(\"word_attn_output\", word_attn_output.size())\n","\n","        #text_logp = self.outputs2vocab(word_lstm_out1.reshape(-1, word_lstm_out1.size(2)))\n","        text_logp = self.outputs2vocab(word_attn_output.reshape(-1, word_attn_output.size(2)))\n","\n","        #print(\"text_logp\", text_logp.shape)\n","        text_logp = text_logp.view(text_b, text_s, 1)\n","\n","\n","        #text_b,text_s,_ = sent_feature.size()\n","        #text_logp = self.outputs2vocab(sent_feature.reshape(-1, sent_feature.size(2)))\n","        #print(\"text_logp\", text_logp.shape)\n","        #text_logp = text_logp.view(text_b, text_s, 1)\n","\n","\n","        #image_feature_topic = torch.cat((image_feature, topic_distance.unsqueeze(1).expand(-1,video_s , -1)), dim=2)\n","        #image_attn_output, _ = self.imageattn(image_feature.permute(1, 0, 2), topic_distance_target.unsqueeze(1).expand(-1,video_s , -1).permute(1, 0, 2), topic_distance_target.unsqueeze(1).expand(-1,video_s , -1).permute(1, 0, 2))\n","        #image_attn_output = self.image_layer_norm(image_attn_output)\n","        #image_attn_output = image_attn_output.permute(1, 0, 2)\n","        #image_feature = self.image_pos_encoder(image_feature)\n","        image_attn_output = self.image_transformer_model(image_feature, topic_distance_target.unsqueeze(1).expand(-1,video_s , -1))\n","        #image_lstm_out = torch.cat([image_feature, image_attn_output.permute(1, 0, 2)], -1)\n","        #image_lstm_out1, (_, _) = self.imagelstm(image_lstm_out)\n","        print(\"image_attn_output\", image_attn_output.size())\n","\n","        #video_logp = self.outputs2coverframe(image_lstm_out1.reshape(-1, image_lstm_out1.size(2)))\n","        video_logp = self.outputs2coverframe(image_attn_output.reshape(-1, image_attn_output.size(2)))\n","        #print(\"video_logp\", video_logp.shape)\n","\n","        video_logp = video_logp.view(video_b, video_s, 1)\n","\n","\n","        output_video_summaries = []\n","        output_video_summaries_pos = []\n","\n","        for image, summary in zip(scene_frame_pad_batch_list, video_logp):\n","            #print(\"summary video\", summary.size())\n","            #output_video_summaries_pos.append(summary.argmax(dim=-1).item())\n","            #rank = torch.topk(summary.squeeze(), self.max_summary_pic).indices\n","            rank = torch.argsort(summary, dim=0, descending=True)\n","            #print(\"rank video\", rank[0])\n","            print(\"image\", len(image))\n","            output_video_summaries_pos.append(rank[0])\n","            output_video_summaries.append(image[int(rank[0])])\n","\n","        #print(\"output_video_summaries_pos\", output_video_summaries_pos)\n","\n","        output_text_summaries = []\n","        output_text_summaries_pos = []\n","\n","        for text, t_id, summary in zip(text_token_list, text_id_list, text_logp):\n","            word_count = 0\n","            pos = []\n","            #print(\"summary text\", summary.size())\n","            #print( \"text\", text)\n","            #rank = torch.topk(summary.squeeze(), self.max_summary_word).indices\n","            rank = torch.argsort(summary, dim=0, descending=True)\n","            #print(\"rank text\", rank)\n","            text_id = []\n","\n","            filtered_rank = []\n","            #for i in sorted(rank):\n","            for i in rank:\n","                if i \u003c len(text) and t_id[int(i)] \u003c 49406:\n","                    filtered_rank.append(int(i))\n","                    if text[int(i)] != PAD_TOKEN and text[int(i)].isalnum():\n","                        word_count += 1\n","\n","                if word_count \u003e self.max_summary_word:\n","                    break\n","\n","            for i in sorted(filtered_rank):\n","                text_id.append(text[int(i)])\n","                pos.append(int(i))\n","\n","            #print(\"text_id\", text_id)\n","            output_text_summaries.append(\" \".join(text_id))\n","            output_text_summaries_pos.append(pos)\n","\n","\n","        #for text, summary in zip(batch_sent_pad, text_logp):\n","            #pos = []\n","        #    print(\"summary text\", summary)\n","        #    print( \"text\", text)\n","            #rank = torch.argsort(summary.squeeze(), descending=True)\n","        #    rank = torch.argsort(summary, dim=0, descending=True)\n","         #   print(\"rank text\", rank)\n","            #text_id = []\n","\n","            #for i in sorted(rank):\n","            #    text_id.append(text[int(i)])\n","            #    pos.append(int(i))\n","            #print(\"text\", text)\n","            #print(\"summary\", summary)\n","            #print(\"rank\", rank)\n","          #  text_summary = ' '\n","           # text_summary_pos = 0\n","\n","            #for rank_i in rank:\n","             #   if text[rank_i] != ' ':\n","              #      text_summary = text[rank_i]\n","               #     text_summary_pos = rank_i\n","                #    break\n","\n","            #output_text_summaries.append(text_summary)\n","            #output_text_summaries_pos.append(text_summary_pos)\n","\n","        print(\"output_text_summaries\", output_text_summaries)\n","        print(\"output_text_summaries_pos\", output_text_summaries_pos)\n","\n","\n","        return output_text_summaries, output_text_summaries_pos, text_logp, output_video_summaries, output_video_summaries_pos, video_logp\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Bs4zfKq0OD_j"},"outputs":[],"source":["from transformers import GPT2LMHeadModel, GPT2Config\n","\n","import torch.utils.data.dataset\n","# import utils_tokenizer\n","import torch, tqdm, math\n","\n","def pad(data, padval=0):\n","    return torch.nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=padval)\n","\n","class GeneTransformer:\n","    def __init__(self, max_output_length=25, max_input_length=300, device='cpu', tokenizer_type='gpt2', bpe_model=\"\", starter_model=None, word_count=None):\n","        if tokenizer_type == \"gpt2\":\n","            self.tokenizer = GPT2Tokenizer()\n","            config = GPT2Config.from_pretrained(\"gpt2\")\n","\n","        elif tokenizer_type == \"bpecap\":\n","            self.tokenizer = BPETokenizer(bpe_model)\n","            config = GPT2Config.from_dict({\"finetuning_task\": None, \"initializer_range\": 0.02,\n","                            \"layer_norm_epsilon\": 1e-05, \"n_ctx\": 1024, \"n_embd\": 768, \"n_head\": 12, \"n_layer\": 12, \"n_positions\": 1024, \"num_labels\": 1,\n","                            \"resid_pdrop\": 0.1, \"use_bfloat16\": False, \"vocab_size\": self.tokenizer.vocab_size})\n","        else:\n","            print(\"Tokenizer unrecognized. Should be gpt2 or bpecap.\")\n","            exit()\n","\n","        self.model = GPT2LMHeadModel(config)\n","\n","        self.model.to(device)\n","        #self.model.cuda(3)\n","        self.device = device\n","        if starter_model is not None:\n","            self.reload(starter_model)\n","\n","        self.max_output_length = max_output_length\n","        self.max_input_length = max_input_length\n","\n","        self.model.train()\n","        self.mode = \"train\"\n","        if word_count is not None:\n","            self.word_count = word_count\n","\n","    def train_batch(self, bodies, summaries, special_append=None, no_preinput=False):\n","        # if self.mode != 'train':\n","        #     print(\"BEWARE. Model is not in train mode.\")\n","\n","        inputs, summ_inp, summ_out = self.preprocess_batch(bodies, summaries, special_append)\n","        past = None\n","        if not no_preinput:\n","           # _, past = self.model(input_ids=inputs, past_key_values=None)\n","            _, past = self.model(input_ids=inputs, past=None)\n","        #logits, _ = self.model(input_ids=summ_inp, past_key_values=past)\n","        logits, _ = self.model(input_ids=summ_inp, past=past)\n","        crit = torch.nn.CrossEntropyLoss(ignore_index=-1)\n","        loss = crit(logits.view(-1, self.tokenizer.vocab_size), summ_out.contiguous().view(-1))\n","        return loss\n","\n","    def train(self):\n","        self.model.train()\n","        self.mode = 'train'\n","\n","    def eval(self):\n","        self.model.eval()\n","        self.mode = 'eval'\n","\n","    def reload(self, from_file):\n","        print(self.model.load_state_dict(torch.load(from_file)))\n","\n","    def save(self, to_file):\n","        torch.save(self.model.state_dict(), to_file)\n","\n","    def preprocess_input(self, bodies, special_append=None):\n","        if special_append is None:\n","            special_append = [[] for i in range(len(bodies))]\n","        inputs = [torch.LongTensor(spe+self.tokenizer.encode(body)) for body, spe in zip(bodies, special_append)]\n","        inputs = pad(inputs, padval=0)\n","        inputs = inputs[:, :self.max_input_length].to(self.device)\n","        return inputs\n","\n","    def preprocess_batch(self, bodies, summaries, special_append=None):\n","        inputs = self.preprocess_input(bodies, special_append)\n","\n","        # Big hack\n","        if special_append is None:\n","            special_append = [[] for i in range(len(bodies))]\n","\n","        summaries = [spe+self.tokenizer.encode(summ) for summ, spe in zip(summaries, special_append)]\n","\n","        summaries = [summ[:(self.max_output_length-1)] for summ in summaries] # We cut short, but we want the end token at the end\n","\n","        summ_inp = pad([torch.LongTensor([self.tokenizer.start_id]+summ) for summ in summaries], padval=0).to(self.device)\n","        summ_out = pad([torch.LongTensor(summ+[self.tokenizer.end_id]) for summ in summaries], padval=-1).to(self.device)\n","        # summ_inp = summ_inp[:, :self.max_output_length].to(self.device)\n","        # summ_out = summ_out[:, :self.max_output_length].to(self.device)\n","        return inputs, summ_inp, summ_out\n","\n","    def score(self, summaries, video_sum, bodies, videos, idx_batch=None, bodies_tokenized=None, lengths=None, extra=None):\n","        # Unconditional rating of the summaries\n","        self.model.eval()\n","        # if self.mode != 'eval':\n","        #     print(\"BEWARE. Model is not in eval mode.\")\n","\n","        inputs, summ_inp, summ_out = self.preprocess_batch(bodies, summaries)\n","        summ_out = summ_out.contiguous()\n","\n","        with torch.no_grad():\n","            #logits, _ = self.model(input_ids=summ_inp[:1024], past=None)\n","            #print(\"summ_inp[:1024]\", summ_inp[:1024])\n","            out = self.model(input_ids=summ_inp[:1024])\n","            #print(\"out\", out)\n","            logits = out[\"logits\"]\n","            #print(\"logits\", logits)\n","            crit = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n","            loss = crit(logits.view(-1, self.tokenizer.vocab_size), summ_out.view(-1)).view(summ_out.shape)\n","            mask = (summ_inp != torch.LongTensor([0]).to(self.device)).float()\n","            non_pad_count = torch.sum(mask, dim=1)\n","            #print(\"summaries\", summaries)\n","\n","            #p_us = []\n","            #total_word = sum(self.word_count.values())\n","\n","            #for idx, summary in enumerate(summaries):\n","            #    tokens = self.tokenizer.encode(' '.join(summary))\n","\n","                #print(\"tokens\", tokens)\n","             #   p_u = 1\n","\n","                #print(\"total_word\", total_word)\n","             #   for token in tokens:\n","                    #print(\"token\", token)\n","             #       try:\n","             #           p_u *= self.word_count[token]/total_word\n","             #       except:\n","             #           p_u *= 1 /total_word#in case the word is not found in the training dataset\n","\n","                #print(\"p_u\", p_u)\n","                #print(\"p_u log\", math.log(p_u+0.001))\n","             #   p_us.append(math.log(p_u+0.001))\n","\n","\n","        #print(\"torch.sum(loss, dim=1)\", torch.sum(loss, dim=1))\n","        #p_us = torch.tensor(p_us).to(self.device)\n","        #print(\"p_us\", p_us)\n","        #loss_per = (torch.sum(loss, dim=1) - p_us)/ non_pad_count\n","        loss_per = torch.sum(loss, dim=1) / non_pad_count\n","\n","        #print(\"loss_per\", loss_per)\n","\n","        #score = (10.0 - loss_per) / 10.0\n","        score = (10.0 - loss_per) / 10.0\n","        #print(\"score\", score)\n","        #score = loss_per\n","        return score, None\n","\n","    def score_pairs(self, bodies, summaries):\n","        if self.mode != 'eval':\n","            print(\"BEWARE. Model is not in eval mode.\")\n","\n","        inputs, summ_inp, summ_out = self.preprocess_batch(bodies, summaries)\n","\n","        with torch.no_grad():\n","            _, past = self.model(input_ids=inputs, past=None)\n","            logits, _ = self.model(input_ids=summ_inp, past=past)\n","\n","            crit = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n","            loss = crit(logits.view(-1, self.tokenizer.vocab_size), summ_out.view(-1)).view(summ_out.shape)\n","            mask = (summ_inp != torch.LongTensor([0]).to(self.device)).float()\n","            non_pad_count = torch.sum(mask, dim=1)\n","            loss_per = torch.sum(loss, dim=1) / non_pad_count\n","\n","        return loss_per.tolist(), None\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"D0JlWAtjOZE0"},"outputs":[{"name":"stdout","output_type":"stream","text":["please use environment variable to specify data directories\n","running\n"]},{"name":"stderr","output_type":"stream","text":["usage: colab_kernel_launcher.py [-h] [--max_sequence_length MAX_SEQUENCE_LENGTH]\n","                                [--max_article_length MAX_ARTICLE_LENGTH]\n","                                [--max_summary_pic MAX_SUMMARY_PIC]\n","                                [--max_summary_word MAX_SUMMARY_WORD] [--test] [-bs BATCH]\n","                                [-lr LEARNING_RATE] [-ths TEXT_HIDDEN_SIZE]\n","                                [-vhs VIDEO_HIDDEN_SIZE] --path PATH --model_dir MODEL_DIR\n","                                --model_name MODEL_NAME [--dataset_folder DATASET_FOLDER]\n","                                [--last_process LAST_PROCESS]\n","colab_kernel_launcher.py: error: the following arguments are required: --path, --model_dir, --model_name\n"]},{"ename":"SystemExit","evalue":"2","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n","\n"]}],"source":["\"\"\" run decoding of rnn-ext + abs + RL (+ rerank)\"\"\"\n","import argparse\n","import json\n","import os\n","from os.path import join, exists\n","from datetime import timedelta\n","from time import time\n","import pickle as pkl\n","from collections import Counter, defaultdict\n","from itertools import product\n","from functools import reduce\n","import operator as op\n","from torch.autograd import Variable\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","\n","from torch import multiprocessing as mp, nn\n","#from utils import make_vocab\n","#from data.batcher import tokenize\n","# from clipsumcomp_topic_trans_3stream_modal_specific import CLIPSum\n","# from clipsum import CLIPSum\n","# from datasets import load_dataset\n","# from util_dataset import EXMSMODataset\n","import cv2\n","\n","try:\n","    DATA_DIR = os.environ['DATA']\n","\n","except KeyError:\n","    print('please use environment variable to specify data directories')\n","\n","def collate_func(inps):\n","    return [a for a in inps]\n","\n","def decode(params, dataset_folder, save_path, model_dir, model_name, split, batch_size, cuda):\n","    start = time()\n","\n","    summarizer = CLIPSum(**params)\n","    print(\"-----------summarizer\")\n","    #print(\"summarizer\", summarizer)\n","    summarizer.cuda()\n","\n","\n","    summarizer.load_state_dict(torch.load(join(model_dir,model_name)))\n","    summarizer.eval()\n","\n","    dataset = EXMSMODataset('test', dataset_folder)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n","    n_data = len(dataset)\n","\n","    # prepare save paths and logs\n","    if not exists(join(save_path, 'outputText')):\n","        os.makedirs(join(save_path, 'outputText'))\n","\n","    if not exists(join(save_path, 'outputTextExtIdx')):\n","        os.makedirs(join(save_path, 'outputTextExtIdx'))\n","\n","    if not exists(join(save_path, 'outputPic')):\n","        os.makedirs(join(save_path, 'outputPic'))\n","\n","    if not exists(join(save_path, 'outputPicExtIdx')):\n","        os.makedirs(join(save_path, 'outputPicExtIdx'))\n","\n","    print(\"loader length\", len(dataloader))\n","    # Decoding\n","    i = 0\n","    start_process=False\n","    if args.last_process == \"\":\n","        start_process=True\n","\n","    print(\"start_process\", start_process)\n","    with torch.no_grad():\n","        for ib, d in enumerate(dataloader):\n","\n","\n","            file_id = d[0][0]\n","\n","            if  args.last_process == file_id:\n","                start_process=True\n","            if start_process:\n","                descriptions = d[1]\n","                videos = d[2]\n","                #scenes = d[6]\n","                #titles.append(d[3])\n","                #transcripts.append(d[4])\n","                print(\"file_id\", file_id)\n","                print(\"descriptions\", descriptions)\n","                print(\"videos\", videos.size())\n","                # Forward pass\n","                #bodies = [doc[args.dataset_doc_field] for doc in documents]\n","                output_text_summaries, output_text_summaries_pos, text_logp, output_video_summaries, output_video_summaries_pos, video_logp = summarizer(descriptions, videos)\n","\n","                    #sampled_summaries, _, sampled_pointers = summarizer.forward(bodies, args.max_ext_output_length, args.max_comp_output_length)\n","\n","\n","                #print(\"decoded\", comp_sampled_summaries[0])\n","\n","                #print(\"decoded ext_arts_w\", ext_arts_w)\n","                with open(join(save_path, 'outputText/{}.dec'.format(file_id)),'w') as f:\n","                    f.write(output_text_summaries[0])\n","\n","                with open(join(save_path, 'outputTextExtIdx/{}.dec'.format(file_id)),'w') as f:\n","                    f.write(','.join([str(sent) for sent in output_text_summaries_pos[0]] ))\n","\n","                #print(\"videos[pics[0]]\", videos[pics[0]].numpy().shape)\n","                #cv2.imwrite(join(save_path, 'outputPic/{}.png'.format(file_id)), cv2.cvtColor(output_video_summaries[0].numpy(), cv2.COLOR_BGR2RGB))\n","                cv2.imwrite(join(save_path, 'outputPic/{}.png'.format(file_id)), output_video_summaries[0].numpy())\n","\n","                #with open(join(args.save_path, 'outputPic/{}.dec'.format(ib)),'w') as f:\n","                #    f.write(videos[pics[0]])\n","                with open(join(save_path, 'outputPicExtIdx/{}.dec'.format(file_id)),'w') as f:\n","                    f.write(','.join([str(pic) for pic in output_video_summaries_pos] )) #adjust position\n","\n","                i += 1\n","    print()\n","\n","\n","\n","\n","if __name__ == '__main__':\n","    print(\"running\")\n","    parser = argparse.ArgumentParser()\n","\n","\n","    parser.add_argument('--max_sequence_length', type=int, default=900)\n","    parser.add_argument('--max_article_length', type=int, default=5)\n","    parser.add_argument('--max_summary_pic', type=int, default=1)\n","    parser.add_argument('--max_summary_word', type=int, default=12)\n","\n","    parser.add_argument('--test', action='store_true')\n","    parser.add_argument('-bs', '--batch', type=int, default=1)\n","    parser.add_argument('-lr', '--learning_rate', type=float, default=0.00005)\n","\n","    parser.add_argument('-ths', '--text_hidden_size', type=int, default=128)\n","    parser.add_argument('-vhs', '--video_hidden_size', type=int, default=128)\n","    #parser.add_argument('-nah', '--num_attention_head', type=int, default=2)\n","\n","\n","    #parser.add_argument('-chs', '--conductor_hidden_size', type=int, default=256)\n","    #parser.add_argument('-dhs', '--decoders_hidden_size', type=int, default=64)\n","    #parser.add_argument('-dis', '--decoders_initial_size', type=int, default=32)\n","\n","    #parser.add_argument('-nl', '--num_layers', type=int, default=2)\n","\n","    parser.add_argument('--path', required=True, help='path to ext model')\n","    parser.add_argument('--model_dir', required=True, help='path to ext model')\n","    parser.add_argument('--model_name', required=True, help='ext model')\n","    parser.add_argument('--dataset_folder', type=str, help='folder of dataset')\n","    parser.add_argument('--last_process', type=str, default='')\n","    args = parser.parse_args()\n","\n","    #args.cuda = torch.cuda.is_available() and not args.no_cuda\n","    print(\"torch.cuda.is_available()\", torch.cuda.is_available())\n","    args.cuda = True\n","\n","    params = dict(\n","        max_summary_word=args.max_summary_word,\n","        max_summary_pic=args.max_summary_pic,\n","        text_hidden_size=args.text_hidden_size,\n","        video_hidden_size=args.video_hidden_size,\n","        #num_attention_head=args.num_attention_head,\n","        #num_layers=args.num_layers,\n","    )\n","    data_split = 'test' if args.test else 'val'\n","    decode(params, args.dataset_folder, args.path, args.model_dir, args.model_name,\n","           data_split, args.batch,\n","           args.cuda)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YCz1gxRlOkcN"},"outputs":[],"source":["import sys\n","#site_packages = next(p for p in sys.path if 'site-packages' in p)\n","#print(site_packages)\n","\n","import numpy as np\n","import os, shutil\n","import re\n","import codecs\n","import os, pickle as pkl\n","# from util_dataset import EXMSMODataset\n","#from MMVAE import MMVAE\n","#from clipsumcomp import CLIPSum\n","#from clipsumcomp_topic import CLIPSum\n","# from clipsum import CLIPSum\n","import codecs\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","from torch import optim\n","from torch import save\n","from torch.nn import functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","# from utils import make_vocab, make_embedding, convert_word2id, to_var, idx2word\n","#from transformers import BertModel\n","from transformers import BertTokenizer\n","from transformers import CLIPFeatureExtractor, CLIPModel, CLIPTokenizer\n","from torchvision import transforms\n","import os\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import gensim\n","import torch.nn.utils.rnn as rnn_utils\n","from multiprocessing import cpu_count\n","import time\n","import torch, gc\n","from collections import OrderedDict, defaultdict\n","# from utils import PAD, UNK, END, START\n","import json\n","from torch.nn.functional import softplus\n","import torchvision.models as models\n","from datasets import load_dataset\n","import cv2 ,ot\n","import ssl\n","# from model_generator import GeneTransformer\n","from torch.utils.tensorboard import SummaryWriter\n","gc.collect()\n","\n","\n","ssl._create_default_https_context = ssl._create_unverified_context\n","BERT_NUM_TOKEN = 30522\n","torch.manual_seed(12345)\n","\n","\n","\n","class TextCoverageLoss:\n","    # Depending on how many words are used a large fraction of the last X summaries\n","    def __init__(self, device=\"cuda\", costmatrix_filename=\"COST_MATRIX_bert.pickle\"):\n","    #def __init__(self, device=\"cpu\", costmatrix_filename=\"COST_MATRIX.pickle\"):\n","\n","        #self.model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)\n","        #self.model.eval()\n","        #self.tokenizer = utils_tokenizer.GPT2Tokenizer()\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        max_bytes = 2**31 - 1\n","\n","        bytes_in = bytearray(0)\n","        input_size = os.path.getsize(costmatrix_filename)\n","        with open(costmatrix_filename, 'rb') as f_in:\n","            for _ in range(0, input_size, max_bytes):\n","                bytes_in += f_in.read(max_bytes)\n","\n","            self.COST_MATRIX = pkl.loads(bytes_in)\n","            #self.COST_MATRIX = pkl.load(f_in, map_location=torch.device('cpu'))\n","            #self.COST_MATRIX = torch.load(f_in, map_location=torch.device('cpu'))\n","        #self.COST_MATRIX = np.negative(self.COST_MATRIX)\n","        #self.COST_MATRIX = np.reciprocal(self.COST_MATRIX)\n","\n","    def score(self, summaries, bodies):\n","        scores = []\n","         # Avoid changing p and q outside of this function\n","        with torch.no_grad():\n","            for i in range(len(summaries)):\n","\n","                #doc = remove_stopwords(bodies[i])\n","                #summary = remove_stopwords(summaries[i])\n","                summary = summaries[i]\n","                doc = bodies[i]\n","                if len(summary)==0:\n","                    score = 1\n","                else:\n","\n","                    summary_token = self.tokenizer.encode(summary)\n","                    body_token = self.tokenizer.encode(doc)\n","\n","                    summary_bow = construct_BOW(summary_token)\n","                    body_bow = construct_BOW(body_token)\n","\n","                    score = sparse_ot(summary_bow, body_bow, self.COST_MATRIX)\n","\n","                scores.append(score)\n","\n","        print('text coverage score', scores)\n","        return sum(scores)/len(scores)\n","\n","\n","class MmCoverageLoss:\n","    def __init__(self, device=\"cuda\"):\n","        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.featureextractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.clip.eval()\n","        self.clip.cuda()\n","        COSTMATRIX_DIM = 512\n","        #self.cosloss = nn.CosineEmbeddingLoss()\n","        self.COST_MATRIX = torch.ones(COSTMATRIX_DIM, COSTMATRIX_DIM) -  torch.eye(COSTMATRIX_DIM)\n","        self.COST_MATRIX = self.COST_MATRIX/COSTMATRIX_DIM\n","\n","\n","    def score(self, text_summaries, video_summaries, texts, videos):\n","        scores = []\n","        #for text, image in zip(text_summaries, video_text_summaries):\n","        #    print(\"text\", text)\n","        #print(\"text_summaries\", text_summaries)\n","        #print(\"video_text_summaries\", video_summaries[0].size())\n","        with torch.no_grad():\n","            for v, t in zip(video_summaries, text_summaries):\n","                print(\"v\", v.size())\n","                #print(\"t\", t.size())\n","                i = transforms.ToPILImage()(v.permute(2,0,1).squeeze_(0))\n","                text_t = self.tokenizer(t, return_tensors = \"pt\", padding=True, truncation=True)\n","                image_t = self.featureextractor(i, return_tensors = \"pt\")\n","                print(\"image_t['pixel_values']\", image_t['pixel_values'].size())\n","                text_f = self.clip.get_text_features(text_t['input_ids'].cuda())\n","                image_f = self.clip.get_image_features(image_t['pixel_values'].cuda())\n","\n","                print(\"text_f\", text_f.size())\n","                print(\"image_f\", image_f.size())\n","                score = sparse_ot(text_f.squeeze(0).cpu().detach().numpy(), image_f.squeeze(0).cpu().detach().numpy(), self.COST_MATRIX.numpy())\n","                scores.append(score)\n","        #scores.append(self.cosloss(text_f, image_f, Variable(torch.ones(text_f.size()[0]).cuda())))\n","        #return scores[0]\n","        #return sum(scores)/len(scores)\n","        return sum(scores)/len(scores)\n","\n","class MmAlignmentLoss:\n","    def __init__(self, device=\"cuda\"):\n","        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.featureextractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        #self.cliptext = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        #self.clipvision = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","       # self.cliptext.eval()\n","        #self.clipvision.eval()\n","\n","        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.clip.eval()\n","        self.clip.cuda()\n","\n","        self.cosloss = nn.CosineEmbeddingLoss()\n","\n","    def score(self, text_summaries, video_summaries):\n","        scores = []\n","        #for text, image in zip(text_summaries, video_text_summaries):\n","        #    print(\"text\", text)\n","        #print(\"text_summaries\", text_summaries)\n","        #print(\"video_text_summaries\", video_summaries[0].size())\n","        text_t = self.tokenizer(text_summaries, return_tensors = \"pt\", padding=True, truncation=True)\n","        #print(\"text_t['input_ids']\", text_t['input_ids'].size())\n","        image_batch = []\n","        for batch in video_summaries:\n","            image_batch.append(transforms.ToPILImage()(batch.permute(2,0,1).squeeze_(0)))\n","\n","        image_t = self.featureextractor(image_batch, return_tensors = \"pt\")\n","        #print(\"image_t['pixel_values']\", image_t['pixel_values'].size())\n","        text_f = self.clip.get_text_features(text_t['input_ids'].cuda())\n","        image_f = self.clip.get_image_features(image_t['pixel_values'].cuda())\n","\n","        #print(\"text_f\", text_f.size())\n","        #print(\"image_f\", image_f.size())\n","\n","        scores.append(self.cosloss(text_f, image_f, Variable(torch.ones(text_f.size()[0]).cuda())))\n","\n","\n","\n","        return scores[0]\n","\n","def VideoCoverageLoss(summaries, bodies):\n","    scores = []\n","    # Avoid changing p and q outside of this function\n","\n","    for summary, video in zip(summaries, bodies):\n","        video = np.mean(np.array(video), axis=0).astype(np.float32)\n","        #summary = np.array(summary).reshape((128, 64)).astype(np.float32)\n","        summary = summary.detach().numpy().astype(np.float32)\n","\n","        #video_64 = cv2.fromarray(video)\n","        #video_32 = cv2.cv.CreateMat(video.rows, video.cols, cv2.CV_32FC1)\n","        #video_32 = np.zeros((video.shape[0], video.shape[1], 1), dtype = np.float32)\n","\n","        #cv2.Convert(video, video_32)\n","\n","        #summary_64 = cv2.fromarray(summary)\n","        #summary_32 = np.zeros((summary_32.shape[0], summary_32.shape[1], 1), dtype = np.float32)\n","\n","        #summary_32 = cv2.cv.CreateMat(summary.rows, summary.cols, cv2.CV_32FC1)\n","        #cv2.Convert(summary, summary_32)\n","\n","        video_bw = cv2.cvtColor(video, cv2.COLOR_BGR2GRAY)\n","        summary_bw = cv2.cvtColor(summary, cv2.COLOR_BGR2GRAY)\n","\n","        #print(\"video_bw\", video_bw.shape)\n","        #print(\"summary_bw\", summary_bw.shape)\n","        #print(\"video_bw\", video_bw)\n","        #print(\"summary_bw\", summary_bw)\n","        score = 1.0 / 0.001\n","        try:\n","        #black_image = cv2.cvtColor((np.ones((256,256,3))*255).astype(np.float32), cv2.COLOR_BGR2GRAY)\n","        #white_image = cv2.cvtColor((np.ones((256,256,3))*0.001).astype(np.float32), cv2.COLOR_BGR2GRAY)\n","\n","        #scale = cv2.EMD(black_image,white_image,cv2.DIST_L2)[0]\n","        #print(\"scale\", scale)\n","    #score = cv2.EMD(summary_bw,video_bw,cv2.DIST_L2)[0] / cv2.EMD(np.ones((500, 500, 1), dtype = \"uint8\")*0.001,np.ones((500, 500, 1), dtype = \"uint8\"),cv2.DIST_L2)[0]\n","            #score = cv2.EMD(summary_bw,video_bw,cv2.DIST_L2)[0] / scale\n","            score = cv2.EMD(summary_bw,video_bw,cv2.DIST_L2)[0]\n","        except:\n","            print(\"VideoCoverageLoss cannot compute\")\n","        scores.append(score)\n","\n","        ## change the latent representation to the actual video/image\n","    print('VideoCoverageLoss', scores)\n","    return sum(scores)/len(scores)\n","\n","class OT_topic():\n","    def __init__(self):\n","        with open(\"kmeans_model.pkl\", \"rb\") as f:\n","            self.k_means =  pkl.load(f)\n","        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.featureextractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        #self.cliptext = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        #self.clipvision = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","       # self.cliptext.eval()\n","        #self.clipvision.eval()\n","\n","        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.clip.eval()\n","        self.clip.cuda()\n","\n","        self.topic_size = 50\n","\n","        self.COST_MATRIX = torch.ones(self.topic_size, self.topic_size) - torch.eye(self.topic_size)\n","        self.COST_MATRIX = self.COST_MATRIX.numpy()\n","\n","    def score(self, summaries_f, bodies_f):\n","        scores = []\n","\n","        topic_distance_summaries = self.k_means.transform(summaries_f.detach().cpu().numpy())\n","        topic_distance_bodies = self.k_means.transform(bodies_f.detach().cpu().numpy())\n","\n","        for summ, bod in zip(topic_distance_summaries, topic_distance_bodies):\n","            score = sparse_ot(summ, bod, self.COST_MATRIX)\n","            scores.append(score)\n","        return scores\n","\n","    def score_text(self, summaries, bodies):\n","\n","        bodies_t = self.tokenizer(bodies, return_tensors = \"pt\", padding=True, truncation=True)\n","        summaries_t = self.tokenizer(summaries, return_tensors = \"pt\", padding=True, truncation=True)\n","\n","        bodies_f = self.clip.get_text_features(bodies_t['input_ids'].cuda())\n","        summaries_f = self.clip.get_text_features(summaries_t['input_ids'].cuda())\n","\n","        scores = self.score(summaries_f, bodies_f)\n","\n","        print('OT topic text coverage score', scores)\n","        return sum(scores)/len(scores)\n","\n","    def score_image(self, summaries, bodies):\n","\n","        summaries_t = self.featureextractor(summaries, return_tensors = \"pt\")\n","        summaries_f = self.clip.get_image_features(summaries_t['pixel_values'].cuda())\n","\n","        videos = []\n","        for b in bodies:\n","            videos.append(torch.from_numpy(np.mean(np.array(b), axis=0).astype(np.float32)))\n","\n","        bodies_t = self.featureextractor(torch.cat(videos), return_tensors = \"pt\")\n","        bodies_f = self.clip.get_image_features(bodies_t['pixel_values'].cuda())\n","\n","        scores = self.score(summaries_f, bodies_f)\n","        print('OT topic visual coverage score', scores)\n","        return sum(scores)/len(scores)\n","\n","    def score_image_text(self, v_summaries, t_summaries):\n","\n","        summaries_vt = self.featureextractor(v_summaries, return_tensors = \"pt\")\n","        summaries_vf = self.clip.get_image_features(summaries_vt['pixel_values'].cuda())\n","\n","        summaries_tt = self.tokenizer(t_summaries, return_tensors = \"pt\", padding=True, truncation=True)\n","        summaries_tf = self.clip.get_text_features(summaries_tt['input_ids'].cuda())\n","\n","        scores = self.score(summaries_vf, summaries_tf)\n","        print('OT topic textual_visual coverage score', scores)\n","        return sum(scores)/len(scores)\n","\n","def save_log(log_input):\n","    file_name = MODEL_PATH + '/log.txt'\n","    p = log_input\n","    c = \"\"\"text_file = open(file_name, \"a+\");text_file.write(p);text_file.close()\"\"\"\n","    exec(c)\n","\n","\n","def main(args):\n","    ts = time.strftime('%Y-%b-%d-%H-%M-%S', time.gmtime())\n","\n","    splits = ['train', 'validation'] + (['test'] if args.test else [])\n","    MODEL_PATH = args.save_model_path\n","    #wv = api.load('word2vec-google-news-300')\n","    dataset_folder = args.dataset_folder\n","\n","\n","    params = dict(\n","        max_summary_word=args.max_summary_word,\n","        max_summary_pic=args.max_summary_pic,\n","        text_hidden_size=args.text_hidden_size,\n","        video_hidden_size=args.video_hidden_size,\n","        #num_attention_head=args.num_attention_head,\n","        #num_layers=args.num_layers,\n","    )\n","\n","\n","    #for i in list(range(9)):\n","    #    train_dataset = EXMSMODataset('train_'+str(i), dataset_folder)\n","    #    with open('train_'+str(i)+'.pickle', 'wb') as handle:\n","    #        pkl.dump(train_dataset, handle , protocol=4)\n","\n","    #    print(\"train_dataset\"+str(i))\n","\n","    #max_bytes = 4096\n","    max_bytes = 2**31 - 1\n","    #train_dataset_list = []\n","\n","    #for i in list(range(9)):\n","    #    bytes_in = bytearray(0)\n","    #    input_size = os.path.getsize('train_'+str(i)+'.pickle')\n","    #    with open('train_'+str(i)+'.pickle', 'rb') as f_in:\n","    #        for _ in range(0, input_size, max_bytes):\n","    #            bytes_in += f_in.read(max_bytes)\n","    #    train_dataset = pkl.loads(bytes_in)\n","    #    train_dataset_list.extend(train_dataset)\n","\n","    #val_dataset = EXMSMODataset('val', dataset_folder)\n","    #with open('val.pickle', 'wb') as handle:\n","    #    pkl.dump(val_dataset, handle , protocol=4)\n","\n","    #bytes_in = bytearray(0)\n","    #input_size = os.path.getsize('test.pickle')\n","    #print(\"input_size\", input_size)\n","    #with open('test.pickle', 'rb') as f_in:\n","     #   unpickler = pkl.Unpickler(f_in)\n","        # if file is not empty scores will be equal\n","        # to the value unpickled\n","    #    train_dataset = unpickler.load()\n","\n","        #print(\"total\", input_size/max_bytes)\n","\n","        #for i in range(0, input_size, max_bytes):\n","        #    print(\"load data\", i/max_bytes)\n","        #    bytes_in += f_in.read(max_bytes)\n","        #train_dataset =    pkl.load(f_in , protocol=4)\n","    #print(\"all loaded\")\n","    #train_dataset = pkl.loads(bytes_in)\n","\n","    def collate_func(inps):\n","        return [a for a in inps]\n","\n","    #del bytes_in\n","\n","    #val_dataset = EXMSMODataset('val_test', dataset_folder)\n","    train_dataset = EXMSMODataset('train', dataset_folder)\n","    val_dataset = EXMSMODataset('val', dataset_folder)\n","    #print(\"train_dataset\", train_dataset)\n","    #print(\"val_dataset\", val_dataset)\n","    print(\"Train Dataset size:\", len(train_dataset))\n","    #print(\"Val Dataset size:\", len(val_dataset))\n","    train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=RandomSampler(train_dataset), drop_last=True, collate_fn=collate_func)\n","    val_data_loader = DataLoader(val_dataset, batch_size=args.batch_size, drop_last=True, collate_fn=collate_func)\n","\n","\n","    model = CLIPSum(**params)\n","\n","    #plain_model = MMHHGATGPO(**params)\n","    #plain_model.load_state_dict(torch.load(\"/share/home/ptan6545/multimodal_OTVAE/mmhhgatgpo_model/2022-Apr-03-01:24:05/E5-1.828360.ckpt\"))\n","\n","    if torch.cuda.is_available():\n","        model = model.cuda()\n","\n","    #for target_param, param in zip(model.parameters(), plain_model.parameters()):\n","    #    target_param.data.copy_(param.data)\n","\n","    if args.resume_training:\n","        model.load_state_dict(torch.load(os.path.join(MODEL_PATH,args.model_name)))\n","        model.train()\n","\n","\n","    print(model)\n","\n","\n","    save_model_path = os.path.join(args.save_model_path, ts)\n","    # save_model_path = os.path.join(args.save_model_path, '1')\n","    os.makedirs(save_model_path)\n","\n","    with open(os.path.join(save_model_path, 'model_params.json'), 'w') as f:\n","        json.dump(params, f, indent=4)\n","\n","    def loss_fn(output_text_summaries, output_video_summaries, texts, videos, OT_topic):\n","\n","\n","        # cut-off unnecessary padding from target, and flatten\n","        #logp = logp[:, :torch.max(summary_length).item(), :].view(-1, logp.size(2))\n","        #logp = logp[:, :summary_length, :].contiguous().view(-1, logp.size(2))\n","        #text_logp = text_logp[:, :, :].contiguous().view(-1, text_logp.size(2))\n","        #print(\"loss target\", target.size())\n","        #print(\"loss logp\", logp.size())\n","        # Negative Log Likelihood\n","\n","        #textcoverage_loss = textCoverageLoss.score(output_text_summaries, texts)\n","        textcoverage_loss = OT_topic.score_text(output_text_summaries, texts)\n","        #videocoverage_loss = VideoCoverageLoss(output_video_summaries, videos)\n","        videocoverage_loss = OT_topic.score_image(output_video_summaries, videos)\n","        #NLL_loss = NLL(logp, target)\n","        #print(\"text_z\", text_z.size())\n","        #print(\"video_z\", video_z.size())\n","        #print('target', Variable(torch.ones(text_z.size()[0])).size())\n","        #mmcoverage_loss = mmCoverageLoss.score(output_text_summaries, output_video_summaries, texts, videos)\n","        #mmcoverage_loss = mmCoverageLoss.score(output_text_summaries, output_video_summaries)\n","        mmcoverage_loss = OT_topic.score_image_text(output_video_summaries, output_text_summaries)\n","        fluency_loss, _ = fluencyLoss.score(output_text_summaries, output_video_summaries, descriptions, videos)\n","        # KL Divergence\n","\n","\n","        return textcoverage_loss, videocoverage_loss , mmcoverage_loss, sum(fluency_loss)/len(fluency_loss)\n","\n","    #optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)\n","    #optimizer = torch.optim.RMSprop(model.parameters(), lr=args.learning_rate)\n","\n","    tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n","    step = 0\n","    fluency_news_model_file = os.path.join(\"models\", \"gpt2_copier23.bin\")\n","\n","    #textCoverageLoss = TextCoverageLoss()\n","    #mmCoverageLoss = MmCoverageLoss()\n","    #mmCoverageLoss = MmAlignmentLoss()\n","    fluencyLoss = GeneTransformer(max_output_length=args.max_summary_word, device=\"cuda\", starter_model=fluency_news_model_file)\n","    otTopic = OT_topic()\n","    writer = SummaryWriter()\n","    for epoch in range(args.epochs):\n","\n","        for split in splits:\n","            print(\"split\", split)\n","            tracker = defaultdict(tensor)\n","\n","            # Enable/Disable Dropout\n","            if split == 'train':\n","                model.train()\n","                data_loader = train_data_loader\n","            else:\n","                model.eval()\n","                data_loader = val_data_loader\n","\n","            #print(\"data_loader\", len(data_loader))\n","\n","            for iteration, data in enumerate(data_loader):\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","                #print(\"iteration\", iteration)\n","                #print(\"data\", data)\n","                print(\"epoch \", epoch)\n","                print(\"iteration \", iteration)\n","                file_id = []\n","                descriptions = []\n","                videos = []\n","                #titles = []\n","                #scenes = []\n","\n","                for d in data:\n","                    file_id.append(d[0])\n","                    descriptions.append(d[1])\n","                    videos.append(d[2])\n","                    #titles.append(d[3])\n","                    #scenes.append(d[6])\n","\n","                batch_size = len(data)\n","\n","                # Forward pass\n","                #bodies = [doc[args.dataset_doc_field] for doc in documents]\n","                output_text_summaries, output_text_summaries_pos, text_logp, output_video_summaries, output_video_summaries_pos, video_logp = model(descriptions, videos)\n","\n","                # loss calculation\n","                textcoverage_loss, videocoverage_loss, mmalignment_loss, fluency_loss = loss_fn(output_text_summaries, output_video_summaries, descriptions, videos, otTopic)\n","\n","                #loss = torch.zeros(1, requires_grad=True)\n","\n","                #loss = (NLL_loss + KL_weight * KL_loss) / batch_size\n","                print(\"textcoverage_loss\", textcoverage_loss)\n","                print(\"videocoverage_loss\", videocoverage_loss)\n","                print(\"mmalignment_loss\", mmalignment_loss)\n","                print(\"fluency_loss\", fluency_loss)\n","                #print(\"batch_size\", batch_size)\n","                #loss = (textcoverage_loss + videocoverage_loss + mmalignment_loss+text_KL_weight * text_KL_loss+video_KL_loss*video_KL_weight) / batch_size\n","                loss = textcoverage_loss + 0.01 * videocoverage_loss + mmalignment_loss + 2* fluency_loss #previous videocoverage 0.001\n","\n","                loss.requires_grad = True\n","                # backward + optimization\n","                if split == 'train':\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    optimizer.step()\n","                    step += 1\n","\n","                # bookkeepeing\n","                tracker['LOSS'] = torch.cat((tracker['LOSS'], loss.data.view(1, -1)), dim=0)\n","\n","                if args.tensorboard_logging:\n","                    writer.add_scalar(\"%s/Loss\" % split.upper(), loss.item(), epoch*len(data_loader) + iteration)\n","                    writer.add_scalar(\"%s/Text Coverage Loss\" % split.upper(), textcoverage_loss,\n","                                      epoch*len(data_loader) + iteration)\n","                    writer.add_scalar(\"%s/Video Coverage Loss\" % split.upper(), videocoverage_loss,\n","                                      epoch*len(data_loader) + iteration)\n","                    writer.add_scalar(\"%s/MM Coverage Loss\" % split.upper(), mmalignment_loss,\n","                                      epoch*len(data_loader) + iteration)\n","\n","                if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n","                    print(\"%s Batch %04d/%i, Loss %9.4f, Text-Coverage-Loss %9.4f, Video-Coverage-Loss %9.4f, MM-Coverage-Loss %9.4f\"\n","                          % (split.upper(), iteration, len(data_loader)-1, loss.item(), textcoverage_loss, videocoverage_loss, mmalignment_loss))\n","\n","                if split == 'valid':\n","                    if 'target_sents' not in tracker:\n","                        tracker['target_sents'] = list()\n","                    #tracker['target_sents'] += idx2word(answer, i2w=datasets['train'].get_i2w(),pad_idx=PAD)\n","                    tracker['target_sents'] += idx2word(answer, i2w=self.id2word,pad_idx=PAD)\n","                    tracker['z'] = torch.cat((tracker['z'], z.data), dim=0)\n","\n","            print(\"%s Epoch %02d/%i, Mean LOSS %9.4f\" % (split.upper(), epoch, args.epochs, tracker['LOSS'].mean()))\n","\n","            if args.tensorboard_logging:\n","                writer.add_scalar(\"%s-Epoch/LOSS\" % split.upper(), torch.mean(tracker['LOSS']), epoch)\n","\n","            # save a dump of all sentences and the encoded latent space\n","            if split == 'valid':\n","                dump = {'target_sents': tracker['target_sents'], 'z': tracker['z'].tolist()}\n","                if not os.path.exists(os.path.join('dumps', ts)):\n","                    os.makedirs('dumps/'+ts)\n","                with open(os.path.join('dumps/'+ts+'/valid_E%i.json' % epoch), 'w') as dump_file:\n","                    json.dump(dump,dump_file)\n","\n","            # save checkpoint\n","            if split == 'train' and epoch%5 ==0:\n","                checkpoint_path = os.path.join(save_model_path, \"E%i-%9f.ckpt\" % (epoch,tracker['LOSS'].mean()))\n","                torch.save(model.state_dict(), checkpoint_path)\n","                print(\"Model saved at %s\" % checkpoint_path)\n","\n","\n","\n","def sparse_ot(weights1, weights2, M):\n","    \"\"\" Compute Wasserstein distances\"\"\"\n","\n","    weights1 = weights1/weights1.sum()\n","    weights2 = weights2/weights2.sum()\n","\n","    active1 = np.where(weights1)[0]\n","    active2 = np.where(weights2)[0]\n","\n","    weights_1_active = weights1[active1]\n","    weights_2_active = weights2[active2]\n","    #print(\"active1\", active1)\n","    #print(\"active2\", active2)\n","    #print(\"M\", M)\n","    #print(\"M\", M)\n","    try1 = M[active1][:,active2]\n","    #print(\"try1\", try1)\n","    M_reduced = np.ascontiguousarray(M[active1][:,active2])\n","\n","    return ot.emd2(weights_1_active,weights_2_active,M_reduced)\n","\n","def construct_BOW(tokens):\n","    bag_vector = np.zeros(BERT_NUM_TOKEN)\n","    for token in tokens:\n","        bag_vector[token] += 1\n","    return bag_vector/len(tokens)\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","\n","\n","    parser.add_argument('--max_sequence_length', type=int, default=900)\n","    parser.add_argument('--max_article_length', type=int, default=5)\n","    parser.add_argument('--max_summary_pic', type=int, default=1)\n","    parser.add_argument('--max_summary_word', type=int, default=12)\n","\n","    parser.add_argument('--test', action='store_true', default='False')\n","\n","    parser.add_argument('-ep', '--epochs', type=int, default=100000)\n","    parser.add_argument('-bs', '--batch_size', type=int, default=2)\n","    parser.add_argument('-lr', '--learning_rate', type=float, default=0.01)\n","\n","    parser.add_argument('-ths', '--text_hidden_size', type=int, default=128)\n","    parser.add_argument('-vhs', '--video_hidden_size', type=int, default=128)\n","    parser.add_argument('-nah', '--num_attention_head', type=int, default=2)\n","    parser.add_argument('-nl', '--num_layers', type=int, default=2)\n","\n","    parser.add_argument('-v', '--print_every', type=int, default=50)\n","    parser.add_argument('-tb', '--tensorboard_logging', action='store_true')\n","    parser.add_argument('-log', '--logdir', type=str, default='logs')\n","    parser.add_argument('-bin', '--save_model_path', type=str, default='multimodal_model')\n","    parser.add_argument('--dataset_folder', type=str, help='folder of dataset', default='/content/drive/MyDrive/Research/data')\n","    parser.add_argument(\"--resume_training\", type=bool, default=False)\n","    parser.add_argument(\"--model_name\", type=str, default='bench')\n","    args = parser.parse_args()\n","\n","\n","    main(args)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qbg72EUOOsvR"},"outputs":[],"source":["!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksY1fugRWIHr"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMvPBtafP5yvt3pxDzgOzYY","gpuType":"V100","mount_file_id":"1McRC936s9vy6MTh9xnakxrFMNpoOebUH","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}