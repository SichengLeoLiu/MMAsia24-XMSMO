{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 60, 512])\n",
      "tgt\n",
      "tensor([[[382.0000, 127.0000,   2.0000,  ..., 334.0000,  35.0000,  48.0000],\n",
      "         [382.0000, 127.0000,   2.0000,  ..., 334.0000,  35.0000,  48.0000],\n",
      "         [382.0000, 127.0000,   2.0000,  ..., 334.0000,  35.0000,  48.0000],\n",
      "         ...,\n",
      "         [382.0000, 127.0000,   2.0000,  ..., 334.0000,  35.0000,  48.0000],\n",
      "         [382.0000, 127.0000,   2.0000,  ..., 334.0000,  35.0000,  48.0000],\n",
      "         [382.0000, 127.0000,   2.0000,  ..., 334.0000,  35.0000,  48.0000]],\n",
      "\n",
      "        [[489.8415, 215.5403, 124.8219,  ..., 100.0000, 266.0001, 163.0000],\n",
      "         [489.8415, 215.5403, 124.8219,  ..., 100.0000, 266.0001, 163.0000],\n",
      "         [489.8415, 215.5403, 124.8219,  ..., 100.0000, 266.0001, 163.0000],\n",
      "         ...,\n",
      "         [489.8415, 215.5403, 124.8219,  ..., 100.0000, 266.0001, 163.0000],\n",
      "         [489.8415, 215.5403, 124.8219,  ..., 100.0000, 266.0001, 163.0000],\n",
      "         [489.8415, 215.5403, 124.8219,  ..., 100.0000, 266.0001, 163.0000]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 60, 512])\n",
      "tensor([[[ 1.9110, -1.4361, -1.1366,  ...,  0.2304, -2.5166, -1.1847],\n",
      "         [ 1.9110, -1.4361, -1.1366,  ...,  0.2304, -2.5166, -1.1847],\n",
      "         [ 1.9110, -1.4361, -1.1366,  ...,  0.2304, -2.5166, -1.1847],\n",
      "         ...,\n",
      "         [ 1.9110, -1.4361, -1.1366,  ...,  0.2304, -2.5166, -1.1847],\n",
      "         [ 1.9110, -1.4361, -1.1366,  ...,  0.2304, -2.5166, -1.1847],\n",
      "         [ 1.9110, -1.4361, -1.1366,  ...,  0.2304, -2.5166, -1.1847]],\n",
      "\n",
      "        [[ 0.6113,  1.1460, -0.0501,  ...,  0.3253, -0.9045,  0.7578],\n",
      "         [ 0.6113,  1.1460, -0.0501,  ...,  0.3253, -0.9045,  0.7578],\n",
      "         [ 0.6113,  1.1460, -0.0501,  ...,  0.3253, -0.9045,  0.7578],\n",
      "         ...,\n",
      "         [ 0.6113,  1.1460, -0.0501,  ...,  0.3253, -0.9045,  0.7578],\n",
      "         [ 0.6113,  1.1460, -0.0501,  ...,  0.3253, -0.9045,  0.7578],\n",
      "         [ 0.6113,  1.1460, -0.0501,  ...,  0.3253, -0.9045,  0.7578]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 假设我们有以下的配置\n",
    "seq_length = 10  # 序列长度\n",
    "batch_size = 2  # 批量大小\n",
    "feature_size = 512  # 特征大小\n",
    "num_heads = 8  # 注意力头数\n",
    "num_encoder_layers = 3  # 编码器层数\n",
    "num_decoder_layers = 3  # 解码器层数\n",
    "\n",
    "# 创建位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# 创建 Transformer 模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=feature_size, nhead=num_heads,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          batch_first=True)\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.input_embedding = nn.Embedding(feature_size, feature_size)\n",
    "        self.output_embedding = nn.Embedding(feature_size, feature_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.input_embedding(src) * torch.sqrt(torch.tensor(feature_size, dtype=torch.float))\n",
    "        # tgt = self.output_embedding(tgt) * torch.sqrt(torch.tensor(feature_size, dtype=torch.float))\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        print(\"tgt\")\n",
    "        print(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        return output\n",
    "\n",
    "# 实例化模型\n",
    "model = TransformerModel().to(device)\n",
    "model.eval()\n",
    "# 创建一些随机数据作为输入和目标序列\n",
    "src = torch.randint(0, feature_size, (batch_size, seq_length)).to(device)\n",
    "tgt = torch.randint(0, feature_size, (batch_size, feature_size)).to(device)\n",
    "tgt = tgt.unsqueeze(1).repeat(1, 60, 1)\n",
    "print(tgt.shape)\n",
    "\n",
    "# 正向传播\n",
    "output = model(src, tgt)\n",
    "\n",
    "# 查看输出形状\n",
    "print(output.shape)  # 应该是 (batch_size, seq_length, feature_size)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设video_embeddings和text_embeddings是模型输出的嵌入向量\n",
    "# video_embeddings和text_embeddings的形状都是[batch_size, sequence_length, hidden_size]\n",
    "# 例如：\n",
    "video_embeddings = torch.randn(2, 20, 128)\n",
    "text_embeddings = torch.randn(2, 20, 128)\n",
    "\n",
    "\n",
    "def split_feature(video_embeddings,text_embeddings):\n",
    "    # 计算余弦相似度\n",
    "    cosine_similarities = F.cosine_similarity(video_embeddings, text_embeddings, dim=2)\n",
    "    # 设置一个相似度阈值\n",
    "    similarity_threshold = 0.1\n",
    "\n",
    "    # 初始化列表来存储不同的信息\n",
    "    multimodal_mutual_info = []\n",
    "    video_only_info = []\n",
    "    text_only_info = []\n",
    "\n",
    "    for batch_idx in range(cosine_similarities.shape[0]):\n",
    "        # 获取当前batch的相似度，并排序\n",
    "        similarities_sorted, indices = torch.sort(cosine_similarities[batch_idx], descending=True)\n",
    "        \n",
    "        # 找到相似度的50%百分位数\n",
    "        threshold_index = 20 // 2\n",
    "        similarity_threshold = similarities_sorted[threshold_index]\n",
    "\n",
    "        multimodal_mutual_info_b = []\n",
    "        video_only_info_b = []\n",
    "        text_only_info_b = []\n",
    "\n",
    "        for seq_idx in range(20):\n",
    "            if cosine_similarities[batch_idx, seq_idx] >= similarity_threshold:\n",
    "                # 如果相似度在前50%，则添加到多模态共有信息中\n",
    "                multimodal_mutual_info_b.append(text_embeddings[batch_idx, seq_idx])\n",
    "                multimodal_mutual_info_b.append(video_embeddings[batch_idx, seq_idx])\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                # 否则，分别添加到模态独有信息中\n",
    "                video_only_info_b.append(video_embeddings[batch_idx, seq_idx])\n",
    "                text_only_info_b.append(text_embeddings[batch_idx, seq_idx])\n",
    "\n",
    "        # 将每个batch的列表转换为Tensor\n",
    "        if multimodal_mutual_info_b:\n",
    "            multimodal_mutual_info.append(torch.stack(multimodal_mutual_info_b))\n",
    "        if video_only_info_b:\n",
    "            video_only_info.append(torch.stack(video_only_info_b))\n",
    "        if text_only_info_b:\n",
    "            text_only_info.append(torch.stack(text_only_info_b))\n",
    "\n",
    "        \n",
    "    # 转换为Tensor\n",
    "    multimodal_mutual_info = torch.stack(multimodal_mutual_info)\n",
    "    video_only_info = torch.stack(video_only_info)\n",
    "    text_only_info = torch.stack(text_only_info)\n",
    "\n",
    "    return multimodal_mutual_info,video_only_info,text_only_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1599,  0.0911,  0.0881,  0.0666,  0.0651,  0.0609,  0.0599,  0.0574,\n",
      "         0.0228,  0.0128,  0.0053,  0.0050, -0.0145, -0.0796, -0.0903, -0.0938,\n",
      "        -0.1380, -0.1483, -0.1800, -0.2523])\n",
      "tensor(0.0053)\n",
      "tensor(0.0574)\n",
      "tensor(0.0881)\n",
      "tensor(0.0651)\n",
      "tensor(0.1599)\n",
      "tensor(0.0609)\n",
      "tensor(0.0599)\n",
      "tensor(0.0666)\n",
      "tensor(0.0128)\n",
      "tensor(0.0911)\n",
      "tensor(0.0228)\n",
      "tensor([ 0.3368,  0.1608,  0.1055,  0.0675,  0.0619,  0.0532,  0.0456,  0.0363,\n",
      "         0.0218,  0.0098, -0.0654, -0.0661, -0.0761, -0.0821, -0.0839, -0.0967,\n",
      "        -0.0999, -0.1007, -0.1070, -0.1594])\n",
      "tensor(0.0098)\n",
      "tensor(0.3368)\n",
      "tensor(0.0218)\n",
      "tensor(0.1055)\n",
      "tensor(0.1608)\n",
      "tensor(0.0456)\n",
      "tensor(0.0619)\n",
      "tensor(-0.0654)\n",
      "tensor(0.0675)\n",
      "tensor(0.0363)\n",
      "tensor(0.0532)\n"
     ]
    }
   ],
   "source": [
    "multimodal_mutual_info,video_only_info,text_only_info=split_feature(video_embeddings,text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22, 128])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_mutual_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 128])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_only_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设video_embeddings和text_embeddings是模型输出的嵌入向量\n",
    "# video_embeddings和text_embeddings的形状都是[batch_size, sequence_length, hidden_size]\n",
    "# 例如：\n",
    "video_embeddings = torch.randn(2, 20, 128)\n",
    "text_embeddings = torch.randn(2, 20, 128)\n",
    "\n",
    "\n",
    "def split_feature(video_embeddings,text_embeddings):\n",
    "    # 计算余弦相似度\n",
    "    # cosine_similarities = F.cosine_similarity(video_embeddings, text_embeddings, dim=2)\n",
    "    v_b,v_l,v_h=video_embeddings.shape\n",
    "    t_b,t_l,t_h=text_embeddings.shape\n",
    "\n",
    "    for batch_idx in range(v_b):\n",
    "        for vl in range(v_l):\n",
    "            for tl in range(t_l):\n",
    "                 cos_sim=F.cosine_similarity(video_embeddings[batch_idx][vl],text_embeddings[batch_idx][tl],dim=0)\n",
    "                 print(cos_sim)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1622)\n",
      "tensor(-0.0991)\n",
      "tensor(-0.0698)\n",
      "tensor(0.0512)\n",
      "tensor(0.0878)\n",
      "tensor(0.1146)\n",
      "tensor(0.0349)\n",
      "tensor(0.0283)\n",
      "tensor(-0.1266)\n",
      "tensor(0.0001)\n",
      "tensor(0.0131)\n",
      "tensor(0.0625)\n",
      "tensor(-0.0445)\n",
      "tensor(0.0094)\n",
      "tensor(0.1458)\n",
      "tensor(-0.1665)\n",
      "tensor(0.1233)\n",
      "tensor(-0.1115)\n",
      "tensor(0.0877)\n",
      "tensor(0.0808)\n",
      "tensor(-0.0807)\n",
      "tensor(0.0544)\n",
      "tensor(0.1224)\n",
      "tensor(-0.0206)\n",
      "tensor(0.0547)\n",
      "tensor(-0.0203)\n",
      "tensor(-0.1266)\n",
      "tensor(-0.0137)\n",
      "tensor(-0.1390)\n",
      "tensor(-0.0266)\n",
      "tensor(-0.1563)\n",
      "tensor(-0.1560)\n",
      "tensor(0.1491)\n",
      "tensor(-0.0549)\n",
      "tensor(0.1059)\n",
      "tensor(0.0905)\n",
      "tensor(-0.1631)\n",
      "tensor(0.1543)\n",
      "tensor(-0.0233)\n",
      "tensor(0.0541)\n",
      "tensor(0.0606)\n",
      "tensor(0.0038)\n",
      "tensor(-0.0537)\n",
      "tensor(0.2433)\n",
      "tensor(-0.0860)\n",
      "tensor(0.0205)\n",
      "tensor(0.0629)\n",
      "tensor(0.0095)\n",
      "tensor(0.1351)\n",
      "tensor(-0.0441)\n",
      "tensor(0.0152)\n",
      "tensor(-0.0495)\n",
      "tensor(-0.0188)\n",
      "tensor(-0.0368)\n",
      "tensor(-0.0115)\n",
      "tensor(0.0271)\n",
      "tensor(-0.0247)\n",
      "tensor(-0.0324)\n",
      "tensor(0.0464)\n",
      "tensor(-0.0895)\n",
      "tensor(0.0607)\n",
      "tensor(0.0108)\n",
      "tensor(0.0306)\n",
      "tensor(-0.0318)\n",
      "tensor(-0.1016)\n",
      "tensor(0.0185)\n",
      "tensor(0.0687)\n",
      "tensor(-0.0774)\n",
      "tensor(-0.0197)\n",
      "tensor(0.0776)\n",
      "tensor(0.0291)\n",
      "tensor(0.0401)\n",
      "tensor(0.1115)\n",
      "tensor(0.1031)\n",
      "tensor(0.0096)\n",
      "tensor(0.0299)\n",
      "tensor(-0.0065)\n",
      "tensor(-0.0242)\n",
      "tensor(0.0047)\n",
      "tensor(0.1867)\n",
      "tensor(0.0763)\n",
      "tensor(-0.0141)\n",
      "tensor(-0.0313)\n",
      "tensor(0.0448)\n",
      "tensor(0.0915)\n",
      "tensor(0.1330)\n",
      "tensor(-0.0743)\n",
      "tensor(0.0176)\n",
      "tensor(0.0646)\n",
      "tensor(-0.0591)\n",
      "tensor(0.0687)\n",
      "tensor(0.0653)\n",
      "tensor(-0.0062)\n",
      "tensor(-0.1698)\n",
      "tensor(0.0682)\n",
      "tensor(0.1327)\n",
      "tensor(0.1044)\n",
      "tensor(-0.0525)\n",
      "tensor(-0.0401)\n",
      "tensor(0.0645)\n",
      "tensor(-0.0355)\n",
      "tensor(-0.1469)\n",
      "tensor(0.1946)\n",
      "tensor(-0.0458)\n",
      "tensor(-0.0590)\n",
      "tensor(0.0358)\n",
      "tensor(0.0201)\n",
      "tensor(0.0421)\n",
      "tensor(0.0005)\n",
      "tensor(0.2052)\n",
      "tensor(-0.1521)\n",
      "tensor(0.0348)\n",
      "tensor(0.0535)\n",
      "tensor(-0.1499)\n",
      "tensor(-0.1438)\n",
      "tensor(0.0485)\n",
      "tensor(0.0409)\n",
      "tensor(0.1680)\n",
      "tensor(-0.0359)\n",
      "tensor(-0.0321)\n",
      "tensor(0.0229)\n",
      "tensor(0.1390)\n",
      "tensor(-0.1110)\n",
      "tensor(-0.0183)\n",
      "tensor(0.0634)\n",
      "tensor(0.1329)\n",
      "tensor(-0.0627)\n",
      "tensor(0.0183)\n",
      "tensor(-0.0179)\n",
      "tensor(0.1490)\n",
      "tensor(0.0323)\n",
      "tensor(0.1702)\n",
      "tensor(0.0462)\n",
      "tensor(0.0390)\n",
      "tensor(-0.0976)\n",
      "tensor(-0.0618)\n",
      "tensor(0.0157)\n",
      "tensor(-0.0322)\n",
      "tensor(-0.0445)\n",
      "tensor(0.1262)\n",
      "tensor(-0.0021)\n",
      "tensor(-0.0528)\n",
      "tensor(0.0267)\n",
      "tensor(0.0292)\n",
      "tensor(-0.0450)\n",
      "tensor(0.0467)\n",
      "tensor(0.0817)\n",
      "tensor(0.0194)\n",
      "tensor(-0.1055)\n",
      "tensor(-0.1282)\n",
      "tensor(-0.0182)\n",
      "tensor(-0.0860)\n",
      "tensor(-0.0711)\n",
      "tensor(-0.0513)\n",
      "tensor(-0.0807)\n",
      "tensor(0.0449)\n",
      "tensor(-0.2565)\n",
      "tensor(-0.0116)\n",
      "tensor(-0.0658)\n",
      "tensor(0.1378)\n",
      "tensor(0.0409)\n",
      "tensor(0.0035)\n",
      "tensor(-0.0379)\n",
      "tensor(-0.0400)\n",
      "tensor(0.1366)\n",
      "tensor(0.0300)\n",
      "tensor(-0.1152)\n",
      "tensor(-0.0193)\n",
      "tensor(0.0482)\n",
      "tensor(-0.1351)\n",
      "tensor(-0.1399)\n",
      "tensor(0.0124)\n",
      "tensor(-0.1706)\n",
      "tensor(-0.1184)\n",
      "tensor(-0.1064)\n",
      "tensor(0.1264)\n",
      "tensor(0.0512)\n",
      "tensor(-0.0078)\n",
      "tensor(0.0014)\n",
      "tensor(-0.0503)\n",
      "tensor(-0.1056)\n",
      "tensor(-0.0749)\n",
      "tensor(-0.0326)\n",
      "tensor(-0.0109)\n",
      "tensor(0.0187)\n",
      "tensor(-0.1488)\n",
      "tensor(-0.0840)\n",
      "tensor(0.0429)\n",
      "tensor(0.1977)\n",
      "tensor(-0.0053)\n",
      "tensor(-0.1371)\n",
      "tensor(0.0553)\n",
      "tensor(-0.0423)\n",
      "tensor(-0.0564)\n",
      "tensor(-0.1191)\n",
      "tensor(0.0264)\n",
      "tensor(-0.0162)\n",
      "tensor(0.0512)\n",
      "tensor(-0.0219)\n",
      "tensor(-0.1682)\n",
      "tensor(0.0171)\n",
      "tensor(-0.0300)\n",
      "tensor(0.0094)\n",
      "tensor(-0.0475)\n",
      "tensor(-0.1834)\n",
      "tensor(0.0509)\n",
      "tensor(-0.1925)\n",
      "tensor(-0.0327)\n",
      "tensor(0.0295)\n",
      "tensor(0.1342)\n",
      "tensor(-0.0826)\n",
      "tensor(-0.0372)\n",
      "tensor(0.0593)\n",
      "tensor(0.1482)\n",
      "tensor(0.0383)\n",
      "tensor(0.0142)\n",
      "tensor(0.0085)\n",
      "tensor(0.0905)\n",
      "tensor(0.0371)\n",
      "tensor(-0.0537)\n",
      "tensor(-0.0087)\n",
      "tensor(0.0095)\n",
      "tensor(0.0034)\n",
      "tensor(0.0766)\n",
      "tensor(-0.0823)\n",
      "tensor(0.1290)\n",
      "tensor(-0.1003)\n",
      "tensor(-0.1073)\n",
      "tensor(0.1962)\n",
      "tensor(0.0816)\n",
      "tensor(0.0860)\n",
      "tensor(0.1521)\n",
      "tensor(0.0125)\n",
      "tensor(0.0620)\n",
      "tensor(0.1148)\n",
      "tensor(-0.1438)\n",
      "tensor(0.0597)\n",
      "tensor(0.0173)\n",
      "tensor(-0.0627)\n",
      "tensor(0.0189)\n",
      "tensor(-0.0073)\n",
      "tensor(-0.1508)\n",
      "tensor(-0.1025)\n",
      "tensor(-0.0357)\n",
      "tensor(-0.0065)\n",
      "tensor(-0.0748)\n",
      "tensor(-0.0018)\n",
      "tensor(-0.0812)\n",
      "tensor(-0.0440)\n",
      "tensor(0.0572)\n",
      "tensor(0.0539)\n",
      "tensor(0.0478)\n",
      "tensor(0.1281)\n",
      "tensor(0.0097)\n",
      "tensor(0.1003)\n",
      "tensor(-0.0670)\n",
      "tensor(-0.1648)\n",
      "tensor(0.0618)\n",
      "tensor(-0.1208)\n",
      "tensor(-0.0167)\n",
      "tensor(-0.0826)\n",
      "tensor(-0.0984)\n",
      "tensor(0.0318)\n",
      "tensor(-0.0310)\n",
      "tensor(-0.1720)\n",
      "tensor(0.0893)\n",
      "tensor(0.0458)\n",
      "tensor(-0.1327)\n",
      "tensor(0.0489)\n",
      "tensor(0.1339)\n",
      "tensor(0.1103)\n",
      "tensor(-0.1467)\n",
      "tensor(-0.0012)\n",
      "tensor(-0.0715)\n",
      "tensor(-0.1377)\n",
      "tensor(-0.1076)\n",
      "tensor(-0.0351)\n",
      "tensor(0.0892)\n",
      "tensor(0.0122)\n",
      "tensor(-0.0148)\n",
      "tensor(0.0175)\n",
      "tensor(-0.0556)\n",
      "tensor(-0.0428)\n",
      "tensor(-0.0704)\n",
      "tensor(-0.1438)\n",
      "tensor(0.1173)\n",
      "tensor(-0.1261)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.1486)\n",
      "tensor(-0.0311)\n",
      "tensor(0.0761)\n",
      "tensor(0.1432)\n",
      "tensor(0.1331)\n",
      "tensor(0.1293)\n",
      "tensor(0.0163)\n",
      "tensor(-0.0897)\n",
      "tensor(0.1450)\n",
      "tensor(0.0728)\n",
      "tensor(0.0936)\n",
      "tensor(-0.0272)\n",
      "tensor(0.0207)\n",
      "tensor(0.0731)\n",
      "tensor(0.1608)\n",
      "tensor(-0.0946)\n",
      "tensor(0.0587)\n",
      "tensor(-0.0631)\n",
      "tensor(-0.1181)\n",
      "tensor(-0.1071)\n",
      "tensor(0.2244)\n",
      "tensor(-0.0800)\n",
      "tensor(-0.1268)\n",
      "tensor(-0.0429)\n",
      "tensor(0.0037)\n",
      "tensor(-0.0362)\n",
      "tensor(0.0454)\n",
      "tensor(0.0994)\n",
      "tensor(0.0862)\n",
      "tensor(0.0810)\n",
      "tensor(0.0058)\n",
      "tensor(0.0715)\n",
      "tensor(-0.0328)\n",
      "tensor(0.1683)\n",
      "tensor(-0.0863)\n",
      "tensor(0.1594)\n",
      "tensor(0.1008)\n",
      "tensor(-0.0862)\n",
      "tensor(-0.1767)\n",
      "tensor(-0.0403)\n",
      "tensor(0.1200)\n",
      "tensor(-0.1899)\n",
      "tensor(0.1576)\n",
      "tensor(0.0427)\n",
      "tensor(0.0316)\n",
      "tensor(-0.0542)\n",
      "tensor(-0.0737)\n",
      "tensor(0.0841)\n",
      "tensor(0.0846)\n",
      "tensor(-0.0103)\n",
      "tensor(0.0265)\n",
      "tensor(-0.1823)\n",
      "tensor(0.0225)\n",
      "tensor(0.0605)\n",
      "tensor(0.0299)\n",
      "tensor(-0.0385)\n",
      "tensor(0.0749)\n",
      "tensor(-0.0747)\n",
      "tensor(-0.1359)\n",
      "tensor(0.0394)\n",
      "tensor(-0.0574)\n",
      "tensor(-0.0178)\n",
      "tensor(0.1323)\n",
      "tensor(-0.0125)\n",
      "tensor(-0.1555)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0080)\n",
      "tensor(0.0086)\n",
      "tensor(-0.0331)\n",
      "tensor(0.1640)\n",
      "tensor(-0.1221)\n",
      "tensor(-0.0471)\n",
      "tensor(0.0740)\n",
      "tensor(0.0151)\n",
      "tensor(0.0883)\n",
      "tensor(0.0801)\n",
      "tensor(0.1778)\n",
      "tensor(-0.0400)\n",
      "tensor(-0.0417)\n",
      "tensor(0.0331)\n",
      "tensor(0.0271)\n",
      "tensor(0.0373)\n",
      "tensor(-0.1160)\n",
      "tensor(-0.1407)\n",
      "tensor(0.0304)\n",
      "tensor(0.1173)\n",
      "tensor(0.0254)\n",
      "tensor(0.1153)\n",
      "tensor(-0.0298)\n",
      "tensor(-0.1805)\n",
      "tensor(0.0994)\n",
      "tensor(-0.0307)\n",
      "tensor(0.0193)\n",
      "tensor(0.0765)\n",
      "tensor(0.0518)\n",
      "tensor(-0.0667)\n",
      "tensor(-0.0567)\n",
      "tensor(-0.0228)\n",
      "tensor(0.2432)\n",
      "tensor(-0.0955)\n",
      "tensor(-0.1141)\n",
      "tensor(0.0739)\n",
      "tensor(-0.0342)\n",
      "tensor(0.0065)\n",
      "tensor(-0.1534)\n",
      "tensor(-0.0258)\n",
      "tensor(0.0266)\n",
      "tensor(0.0149)\n",
      "tensor(-0.0553)\n",
      "tensor(0.0949)\n",
      "tensor(0.0408)\n",
      "tensor(0.0688)\n",
      "tensor(-0.1217)\n",
      "tensor(0.0572)\n",
      "tensor(-0.0856)\n",
      "tensor(0.0379)\n",
      "tensor(-0.0660)\n",
      "tensor(0.0820)\n",
      "tensor(0.0739)\n",
      "tensor(-0.0268)\n",
      "tensor(0.0879)\n",
      "tensor(0.0374)\n",
      "tensor(-0.1332)\n",
      "tensor(-0.2637)\n",
      "tensor(0.1180)\n",
      "tensor(0.0478)\n",
      "tensor(0.0337)\n",
      "tensor(0.1469)\n",
      "tensor(0.0287)\n",
      "tensor(-0.0789)\n",
      "tensor(0.0484)\n",
      "tensor(0.0825)\n",
      "tensor(-0.0538)\n",
      "tensor(0.0143)\n",
      "tensor(-0.1041)\n",
      "tensor(0.0118)\n",
      "tensor(0.0627)\n",
      "tensor(0.1217)\n",
      "tensor(0.0384)\n",
      "tensor(0.0657)\n",
      "tensor(-0.0194)\n",
      "tensor(0.1628)\n",
      "tensor(-0.0293)\n",
      "tensor(-0.0525)\n",
      "tensor(-0.0289)\n",
      "tensor(0.0658)\n",
      "tensor(-0.0654)\n",
      "tensor(0.0423)\n",
      "tensor(-0.0328)\n",
      "tensor(0.0446)\n",
      "tensor(-0.1117)\n",
      "tensor(-0.0765)\n",
      "tensor(-0.1050)\n",
      "tensor(0.0303)\n",
      "tensor(0.0042)\n",
      "tensor(0.1474)\n",
      "tensor(-0.0119)\n",
      "tensor(-0.0714)\n",
      "tensor(0.0828)\n",
      "tensor(-0.0404)\n",
      "tensor(0.0112)\n",
      "tensor(-0.0784)\n",
      "tensor(0.1426)\n",
      "tensor(-0.0542)\n",
      "tensor(0.0218)\n",
      "tensor(0.0259)\n",
      "tensor(-0.0086)\n",
      "tensor(-0.0720)\n",
      "tensor(0.0305)\n",
      "tensor(0.0264)\n",
      "tensor(-0.0161)\n",
      "tensor(-0.1659)\n",
      "tensor(-0.0144)\n",
      "tensor(-0.0415)\n",
      "tensor(-0.0854)\n",
      "tensor(0.0625)\n",
      "tensor(-0.0349)\n",
      "tensor(-0.1136)\n",
      "tensor(0.0009)\n",
      "tensor(0.1050)\n",
      "tensor(0.0298)\n",
      "tensor(0.2098)\n",
      "tensor(0.1165)\n",
      "tensor(0.0255)\n",
      "tensor(-0.1588)\n",
      "tensor(-0.0681)\n",
      "tensor(-0.0207)\n",
      "tensor(0.0533)\n",
      "tensor(-0.0734)\n",
      "tensor(-0.0764)\n",
      "tensor(0.0136)\n",
      "tensor(-0.0181)\n",
      "tensor(-0.0887)\n",
      "tensor(0.0441)\n",
      "tensor(0.0516)\n",
      "tensor(-0.0357)\n",
      "tensor(-0.0225)\n",
      "tensor(-0.1189)\n",
      "tensor(0.0347)\n",
      "tensor(-0.0706)\n",
      "tensor(-0.1526)\n",
      "tensor(0.0455)\n",
      "tensor(0.0456)\n",
      "tensor(0.0904)\n",
      "tensor(-0.0865)\n",
      "tensor(0.1042)\n",
      "tensor(0.0985)\n",
      "tensor(0.0848)\n",
      "tensor(0.0753)\n",
      "tensor(-0.0435)\n",
      "tensor(-0.0387)\n",
      "tensor(-0.1559)\n",
      "tensor(0.0884)\n",
      "tensor(0.1007)\n",
      "tensor(0.0520)\n",
      "tensor(-0.0423)\n",
      "tensor(0.0789)\n",
      "tensor(-0.1822)\n",
      "tensor(-0.0434)\n",
      "tensor(0.0419)\n",
      "tensor(0.0781)\n",
      "tensor(-0.0708)\n",
      "tensor(-0.1234)\n",
      "tensor(0.0258)\n",
      "tensor(0.0391)\n",
      "tensor(-0.0041)\n",
      "tensor(0.1305)\n",
      "tensor(-0.0029)\n",
      "tensor(-0.0567)\n",
      "tensor(0.0580)\n",
      "tensor(-0.1176)\n",
      "tensor(0.1130)\n",
      "tensor(0.0928)\n",
      "tensor(0.1458)\n",
      "tensor(-0.0818)\n",
      "tensor(0.0684)\n",
      "tensor(0.0066)\n",
      "tensor(-0.1222)\n",
      "tensor(-0.3025)\n",
      "tensor(-0.1109)\n",
      "tensor(0.0227)\n",
      "tensor(-0.1462)\n",
      "tensor(-0.0420)\n",
      "tensor(-0.0171)\n",
      "tensor(-0.0648)\n",
      "tensor(-0.0215)\n",
      "tensor(-0.0193)\n",
      "tensor(-0.1439)\n",
      "tensor(-0.0773)\n",
      "tensor(0.0581)\n",
      "tensor(-0.0586)\n",
      "tensor(0.0700)\n",
      "tensor(-0.0389)\n",
      "tensor(0.0899)\n",
      "tensor(0.1019)\n",
      "tensor(-0.0472)\n",
      "tensor(-0.0353)\n",
      "tensor(0.0377)\n",
      "tensor(-0.0586)\n",
      "tensor(-0.0149)\n",
      "tensor(0.1762)\n",
      "tensor(-0.0239)\n",
      "tensor(0.1558)\n",
      "tensor(-0.1298)\n",
      "tensor(0.0381)\n",
      "tensor(0.1267)\n",
      "tensor(0.1291)\n",
      "tensor(-0.1179)\n",
      "tensor(-0.0735)\n",
      "tensor(0.1240)\n",
      "tensor(-0.1169)\n",
      "tensor(-0.1478)\n",
      "tensor(-0.0101)\n",
      "tensor(-0.0447)\n",
      "tensor(-0.0002)\n",
      "tensor(0.0447)\n",
      "tensor(-0.0833)\n",
      "tensor(-0.0710)\n",
      "tensor(0.0178)\n",
      "tensor(0.0594)\n",
      "tensor(0.0355)\n",
      "tensor(0.0276)\n",
      "tensor(-0.0062)\n",
      "tensor(0.0020)\n",
      "tensor(-0.1079)\n",
      "tensor(0.0038)\n",
      "tensor(-0.0934)\n",
      "tensor(-0.0709)\n",
      "tensor(-0.0729)\n",
      "tensor(-0.0291)\n",
      "tensor(-0.0905)\n",
      "tensor(-0.0749)\n",
      "tensor(-0.1020)\n",
      "tensor(-0.0154)\n",
      "tensor(0.1182)\n",
      "tensor(0.0867)\n",
      "tensor(-0.0688)\n",
      "tensor(-0.1039)\n",
      "tensor(0.0085)\n",
      "tensor(0.1344)\n",
      "tensor(-0.2138)\n",
      "tensor(0.0362)\n",
      "tensor(0.1460)\n",
      "tensor(-0.0353)\n",
      "tensor(-0.0110)\n",
      "tensor(0.0630)\n",
      "tensor(-0.1255)\n",
      "tensor(-0.0044)\n",
      "tensor(-0.0125)\n",
      "tensor(-0.0094)\n",
      "tensor(-0.0160)\n",
      "tensor(-0.0276)\n",
      "tensor(0.0737)\n",
      "tensor(0.0286)\n",
      "tensor(0.0390)\n",
      "tensor(-0.0585)\n",
      "tensor(0.0295)\n",
      "tensor(-0.1320)\n",
      "tensor(-0.1733)\n",
      "tensor(0.0320)\n",
      "tensor(0.0417)\n",
      "tensor(-0.0088)\n",
      "tensor(-0.0182)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0776)\n",
      "tensor(-0.1669)\n",
      "tensor(0.0048)\n",
      "tensor(0.0598)\n",
      "tensor(-0.2037)\n",
      "tensor(0.0742)\n",
      "tensor(0.2276)\n",
      "tensor(0.1088)\n",
      "tensor(0.0938)\n",
      "tensor(-0.1353)\n",
      "tensor(0.1046)\n",
      "tensor(-0.0356)\n",
      "tensor(-0.1555)\n",
      "tensor(-0.0532)\n",
      "tensor(-0.0003)\n",
      "tensor(0.1412)\n",
      "tensor(-0.0032)\n",
      "tensor(0.0844)\n",
      "tensor(0.1119)\n",
      "tensor(0.0116)\n",
      "tensor(-0.0501)\n",
      "tensor(-0.0551)\n",
      "tensor(0.0290)\n",
      "tensor(-0.1157)\n",
      "tensor(-0.1602)\n",
      "tensor(0.0075)\n",
      "tensor(0.1679)\n",
      "tensor(0.0024)\n",
      "tensor(0.0683)\n",
      "tensor(0.0140)\n",
      "tensor(0.0618)\n",
      "tensor(0.0891)\n",
      "tensor(0.0901)\n",
      "tensor(-0.0777)\n",
      "tensor(0.0036)\n",
      "tensor(-0.0083)\n",
      "tensor(-0.0281)\n",
      "tensor(-0.0199)\n",
      "tensor(-0.0092)\n",
      "tensor(0.0495)\n",
      "tensor(0.0762)\n",
      "tensor(-0.2381)\n",
      "tensor(-0.0393)\n",
      "tensor(-0.0808)\n",
      "tensor(-0.0089)\n",
      "tensor(0.0162)\n",
      "tensor(-0.0909)\n",
      "tensor(-0.0118)\n",
      "tensor(-0.0406)\n",
      "tensor(0.0977)\n",
      "tensor(-0.0934)\n",
      "tensor(0.0672)\n",
      "tensor(-0.0150)\n",
      "tensor(-0.0185)\n",
      "tensor(0.0712)\n",
      "tensor(-0.0201)\n",
      "tensor(0.0619)\n",
      "tensor(-0.2482)\n",
      "tensor(0.0419)\n",
      "tensor(-0.1417)\n",
      "tensor(0.0934)\n",
      "tensor(0.0576)\n",
      "tensor(0.1584)\n",
      "tensor(-0.0232)\n",
      "tensor(0.1143)\n",
      "tensor(-0.0259)\n",
      "tensor(0.1626)\n",
      "tensor(-0.0928)\n",
      "tensor(-0.0762)\n",
      "tensor(0.2618)\n",
      "tensor(-0.1054)\n",
      "tensor(-0.2719)\n",
      "tensor(0.1407)\n",
      "tensor(-0.1075)\n",
      "tensor(-0.1430)\n",
      "tensor(0.0600)\n",
      "tensor(0.0386)\n",
      "tensor(0.1635)\n",
      "tensor(-0.0424)\n",
      "tensor(0.0020)\n",
      "tensor(-0.1156)\n",
      "tensor(0.0168)\n",
      "tensor(-0.1542)\n",
      "tensor(0.0745)\n",
      "tensor(-0.0580)\n",
      "tensor(-0.0338)\n",
      "tensor(0.0513)\n",
      "tensor(0.0793)\n",
      "tensor(0.1158)\n",
      "tensor(-0.0959)\n",
      "tensor(-0.1787)\n",
      "tensor(0.0528)\n",
      "tensor(0.1130)\n",
      "tensor(0.0502)\n",
      "tensor(0.1560)\n",
      "tensor(-0.0251)\n",
      "tensor(0.0263)\n",
      "tensor(-0.0240)\n",
      "tensor(0.0616)\n",
      "tensor(-0.1054)\n",
      "tensor(0.2216)\n",
      "tensor(0.0818)\n",
      "tensor(0.1254)\n",
      "tensor(-0.0760)\n",
      "tensor(-0.0330)\n",
      "tensor(-0.0145)\n",
      "tensor(-0.0904)\n",
      "tensor(0.0378)\n",
      "tensor(0.1060)\n",
      "tensor(0.1113)\n",
      "tensor(-0.0059)\n",
      "tensor(-0.0613)\n",
      "tensor(0.0773)\n",
      "tensor(-0.0338)\n",
      "tensor(-0.0281)\n",
      "tensor(-0.0767)\n",
      "tensor(0.1680)\n",
      "tensor(0.1441)\n",
      "tensor(0.1607)\n",
      "tensor(0.2262)\n",
      "tensor(-0.0131)\n",
      "tensor(-0.0622)\n",
      "tensor(0.0423)\n",
      "tensor(-0.0221)\n",
      "tensor(-0.1733)\n",
      "tensor(-0.0159)\n",
      "tensor(-0.0333)\n",
      "tensor(-0.1300)\n",
      "tensor(-0.1208)\n",
      "tensor(0.0408)\n",
      "tensor(-0.0833)\n",
      "tensor(0.0073)\n",
      "tensor(0.0376)\n",
      "tensor(-0.0109)\n",
      "tensor(-0.0471)\n",
      "tensor(0.0523)\n",
      "tensor(-0.1780)\n",
      "tensor(-0.1378)\n",
      "tensor(0.0732)\n",
      "tensor(-8.9727e-05)\n",
      "tensor(0.1198)\n",
      "tensor(-0.0194)\n",
      "tensor(-0.0600)\n",
      "tensor(0.0213)\n",
      "tensor(0.0514)\n",
      "tensor(0.0238)\n",
      "tensor(-0.1500)\n",
      "tensor(0.0217)\n",
      "tensor(-0.0682)\n",
      "tensor(-0.0343)\n",
      "tensor(-0.0375)\n",
      "tensor(-0.1419)\n",
      "tensor(-0.0505)\n",
      "tensor(0.1889)\n",
      "tensor(0.0272)\n",
      "tensor(-0.0648)\n",
      "tensor(-0.1053)\n",
      "tensor(-0.0698)\n",
      "tensor(0.0321)\n",
      "tensor(-0.0097)\n",
      "tensor(0.0271)\n",
      "tensor(0.0875)\n",
      "tensor(0.0325)\n",
      "tensor(0.0077)\n",
      "tensor(-0.0050)\n",
      "tensor(0.0364)\n",
      "tensor(-0.0489)\n",
      "tensor(-0.0038)\n",
      "tensor(0.0341)\n",
      "tensor(0.0255)\n",
      "tensor(0.1212)\n",
      "tensor(0.0132)\n",
      "tensor(-0.1110)\n",
      "tensor(0.1301)\n",
      "tensor(0.1853)\n",
      "tensor(0.1276)\n",
      "tensor(0.0317)\n",
      "tensor(0.0448)\n",
      "tensor(0.1260)\n",
      "tensor(-0.0669)\n",
      "tensor(0.0190)\n",
      "tensor(0.0127)\n",
      "tensor(-0.0121)\n",
      "tensor(-0.0982)\n",
      "tensor(0.1313)\n",
      "tensor(-0.2020)\n",
      "tensor(-0.0668)\n",
      "tensor(-0.1788)\n"
     ]
    }
   ],
   "source": [
    "split_feature(video_embeddings,text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5])\n",
      "torch.Size([20, 5])\n",
      "torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "sequence_length = 3  # 序列长度\n",
    "batch_size = 2  # 批量大小\n",
    "hidden_size = 5  # 特征大小\n",
    "def split_feature(video_embeddings, text_embeddings):\n",
    "    # 确保视频和文本嵌入的batch size是相同的\n",
    "    assert video_embeddings.size(0) == text_embeddings.size(0)\n",
    "    \n",
    "    # 获取嵌入维度\n",
    "    v_b, v_l, v_h = video_embeddings.shape\n",
    "    t_b, t_l, t_h = text_embeddings.shape\n",
    "    assert v_h == t_h  # 确保视频和文本嵌入在特征维度上是相同的\n",
    "\n",
    "    # 初始化列表来存储共有信息和各自模态的独有信息\n",
    "    multimodal_mutual_info = []\n",
    "    video_only_info = []\n",
    "    text_only_info = []\n",
    "\n",
    "    # 对每个batch进行处理\n",
    "    for batch_idx in range(v_b):\n",
    "        # 计算当前batch中所有特征对的相似度\n",
    "        similarities = torch.zeros((v_l, t_l))\n",
    "        for vl in range(v_l):\n",
    "            for tl in range(t_l):\n",
    "                # 计算视频特征和文本特征之间的余弦相似度\n",
    "                cos_sim = F.cosine_similarity(video_embeddings[batch_idx][vl].unsqueeze(0), text_embeddings[batch_idx][tl].unsqueeze(0))\n",
    "                similarities[vl, tl] = cos_sim\n",
    "\n",
    "        # 将相似度矩阵转换为一维数组并排序，选择相似度最高的50%\n",
    "        flat_similarities = similarities.view(-1)\n",
    "        sorted_similarities, sorted_indices = torch.sort(flat_similarities, descending=True)\n",
    "        top_half_indices = sorted_indices[:len(sorted_indices) // 2]\n",
    "\n",
    "\n",
    "\n",
    "        # 选取前50%的相似度对应的特征\n",
    "        selected_video_indices = top_half_indices // t_l\n",
    "        selected_text_indices = top_half_indices % t_l\n",
    "\n",
    "\n",
    "        selected_video_indices=torch.unique_consecutive(selected_video_indices)[:vl//2]\n",
    "\n",
    "        selected_text_indices=torch.unique_consecutive(selected_text_indices)[:tl//2]\n",
    "\n",
    "        # 标记所有特征是否已选为共有特征\n",
    "        selected_for_mutual_info = torch.zeros_like(similarities, dtype=torch.bool)\n",
    "\n",
    "        # 标记选为共有特征的位置\n",
    "        selected_for_mutual_info[selected_video_indices, selected_text_indices] = True\n",
    "\n",
    "        # 根据标记将特征分配到对应列表\n",
    "        for vl in range(v_l):\n",
    "            for tl in range(t_l):\n",
    "                if selected_for_mutual_info[vl, tl]:\n",
    "                    # 如果是共有特征\n",
    "                    multimodal_mutual_info.append(video_embeddings[batch_idx, vl])\n",
    "                    multimodal_mutual_info.append(text_embeddings[batch_idx, tl])\n",
    "                else:\n",
    "                    # 分别添加到各自模态的独有信息中\n",
    "                    video_only_info.append(video_embeddings[batch_idx, vl])\n",
    "                    text_only_info.append(text_embeddings[batch_idx, tl])\n",
    "\n",
    "        for vi in range(v_l):\n",
    "            if vi in selected_video_indices:\n",
    "                multimodal_mutual_info.append(video_embeddings[batch_idx, vi])\n",
    "            else:\n",
    "                video_only_info.append(video_embeddings[batch_idx, vi])\n",
    "        for ti in range(t_l):\n",
    "            if ti in selected_text_indices:\n",
    "                multimodal_mutual_info.append(text_embeddings[batch_idx, ti])\n",
    "            else:\n",
    "                text_only_info.append(text_embeddings[batch_idx, ti])\n",
    "\n",
    "\n",
    "    # 转换列表为Tensor\n",
    "    if multimodal_mutual_info:\n",
    "        multimodal_mutual_info = torch.stack(multimodal_mutual_info)\n",
    "    if video_only_info:\n",
    "        video_only_info = torch.stack(video_only_info)\n",
    "    if text_only_info:\n",
    "        text_only_info = torch.stack(text_only_info)\n",
    "    \n",
    "    return multimodal_mutual_info, video_only_info, text_only_info\n",
    "\n",
    "# 假设的视频和文本嵌入向量\n",
    "video_embeddings = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "text_embeddings = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# 调用函数\n",
    "multimodal_mutual_info, video_only_info, text_only_info = split_feature(video_embeddings, text_embeddings)\n",
    "\n",
    "# 检查结果\n",
    "print(multimodal_mutual_info.shape)\n",
    "print(video_only_info.shape)\n",
    "print(text_only_info.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.4921, -1.7299, -3.6645, -0.1130,  0.1598, -0.1183,  0.1616,  1.1894,\n",
      "         1.0763,  1.3261,  0.0129,  0.5039])\n",
      "tensor([ 1.0351,  0.3673,  0.0557, -0.8657, -1.0000, -0.8180, -0.3943, -1.0877,\n",
      "         0.4152, -1.1640, -1.2055,  0.9571])\n",
      "tensor([ 2.4921, -1.7299, -3.6645, -0.1130,  0.1598, -0.1183,  0.1616,  1.1894,\n",
      "         1.0763,  1.3261,  0.0129,  0.5039])\n",
      "tensor([-0.4894, -0.0323, -0.6528,  0.9902, -2.2690,  0.1405,  1.7084, -0.6363,\n",
      "         0.1503, -0.7970, -1.4187,  0.8197])\n",
      "tensor([ 2.4921, -1.7299, -3.6645, -0.1130,  0.1598, -0.1183,  0.1616,  1.1894,\n",
      "         1.0763,  1.3261,  0.0129,  0.5039])\n",
      "tensor([ 0.4048,  0.4500,  0.0289,  0.9127, -0.7483,  1.5466,  3.2040,  1.5057,\n",
      "        -0.4348, -0.0674, -0.3573, -1.6098])\n",
      "tensor([ 0.8275, -1.1033, -0.7473, -1.4598, -0.6643, -0.1592, -0.0334,  1.6658,\n",
      "         0.0148,  1.0321,  0.1353, -0.4821])\n",
      "tensor([ 0.7179,  1.0248,  0.9483, -1.3332, -0.7922,  0.3112,  1.2247,  0.1719,\n",
      "        -1.2398,  1.1800, -1.6752,  0.2876])\n",
      "tensor([ 0.8275, -1.1033, -0.7473, -1.4598, -0.6643, -0.1592, -0.0334,  1.6658,\n",
      "         0.0148,  1.0321,  0.1353, -0.4821])\n",
      "tensor([-0.9583, -0.1644,  1.3647, -1.8202, -0.5029, -0.2993, -0.1899,  0.4448,\n",
      "        -1.1722,  0.8239, -0.0912,  0.1398])\n",
      "tensor([ 0.8275, -1.1033, -0.7473, -1.4598, -0.6643, -0.1592, -0.0334,  1.6658,\n",
      "         0.0148,  1.0321,  0.1353, -0.4821])\n",
      "tensor([ 0.4048,  0.4500,  0.0289,  0.9127, -0.7483,  1.5466,  3.2040,  1.5057,\n",
      "        -0.4348, -0.0674, -0.3573, -1.6098])\n",
      "tensor([-0.6495, -0.5926, -1.2520,  0.4122, -1.7329,  0.8766,  1.5622,  1.2207,\n",
      "        -1.0329,  0.4076,  0.4104, -0.2148])\n",
      "tensor([ 0.7179,  1.0248,  0.9483, -1.3332, -0.7922,  0.3112,  1.2247,  0.1719,\n",
      "        -1.2398,  1.1800, -1.6752,  0.2876])\n",
      "tensor([-0.6495, -0.5926, -1.2520,  0.4122, -1.7329,  0.8766,  1.5622,  1.2207,\n",
      "        -1.0329,  0.4076,  0.4104, -0.2148])\n",
      "tensor([-0.9583, -0.1644,  1.3647, -1.8202, -0.5029, -0.2993, -0.1899,  0.4448,\n",
      "        -1.1722,  0.8239, -0.0912,  0.1398])\n",
      "tensor([-0.6495, -0.5926, -1.2520,  0.4122, -1.7329,  0.8766,  1.5622,  1.2207,\n",
      "        -1.0329,  0.4076,  0.4104, -0.2148])\n",
      "tensor([-0.4394,  1.6873,  0.9231,  0.3819, -1.5698,  1.2345,  0.7134, -0.4158,\n",
      "         0.6749,  1.1299,  0.3694, -0.1149])\n",
      "tensor([-0.6495, -0.5926, -1.2520,  0.4122, -1.7329,  0.8766,  1.5622,  1.2207,\n",
      "        -1.0329,  0.4076,  0.4104, -0.2148])\n",
      "tensor([-0.4894, -0.0323, -0.6528,  0.9902, -2.2690,  0.1405,  1.7084, -0.6363,\n",
      "         0.1503, -0.7970, -1.4187,  0.8197])\n",
      "tensor([-0.6495, -0.5926, -1.2520,  0.4122, -1.7329,  0.8766,  1.5622,  1.2207,\n",
      "        -1.0329,  0.4076,  0.4104, -0.2148])\n",
      "tensor([-0.9350,  0.4967,  1.2768,  0.7883,  0.3773,  1.6894,  0.9493, -1.2904,\n",
      "         0.2382,  1.3618,  0.9784, -0.8932])\n",
      "tensor([-0.6495, -0.5926, -1.2520,  0.4122, -1.7329,  0.8766,  1.5622,  1.2207,\n",
      "        -1.0329,  0.4076,  0.4104, -0.2148])\n",
      "tensor([-1.5437, -0.7408,  0.9241,  1.0720,  0.1766, -0.5238,  0.4340, -0.4353,\n",
      "        -1.1127,  1.4668, -0.9486, -0.6758])\n",
      "tensor([-0.6495, -0.5926, -1.2520,  0.4122, -1.7329,  0.8766,  1.5622,  1.2207,\n",
      "        -1.0329,  0.4076,  0.4104, -0.2148])\n",
      "tensor([ 0.4048,  0.4500,  0.0289,  0.9127, -0.7483,  1.5466,  3.2040,  1.5057,\n",
      "        -0.4348, -0.0674, -0.3573, -1.6098])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([-1.0081, -0.1326,  0.0402,  0.2281,  0.3770, -0.1139, -2.2329,  0.4262,\n",
      "        -0.3503, -0.9822, -0.1224,  1.1088])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([ 0.7179,  1.0248,  0.9483, -1.3332, -0.7922,  0.3112,  1.2247,  0.1719,\n",
      "        -1.2398,  1.1800, -1.6752,  0.2876])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([-0.9583, -0.1644,  1.3647, -1.8202, -0.5029, -0.2993, -0.1899,  0.4448,\n",
      "        -1.1722,  0.8239, -0.0912,  0.1398])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([-0.4394,  1.6873,  0.9231,  0.3819, -1.5698,  1.2345,  0.7134, -0.4158,\n",
      "         0.6749,  1.1299,  0.3694, -0.1149])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([ 0.1241, -0.2803,  0.6302,  0.4358, -0.4561, -1.1573,  0.1540, -0.0535,\n",
      "         1.0204, -0.9439, -0.4562,  0.2757])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([-0.4894, -0.0323, -0.6528,  0.9902, -2.2690,  0.1405,  1.7084, -0.6363,\n",
      "         0.1503, -0.7970, -1.4187,  0.8197])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([-0.9350,  0.4967,  1.2768,  0.7883,  0.3773,  1.6894,  0.9493, -1.2904,\n",
      "         0.2382,  1.3618,  0.9784, -0.8932])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([-1.5437, -0.7408,  0.9241,  1.0720,  0.1766, -0.5238,  0.4340, -0.4353,\n",
      "        -1.1127,  1.4668, -0.9486, -0.6758])\n",
      "tensor([-2.8387,  1.5473,  0.5294,  1.8634, -0.5192, -0.1812,  1.0784,  0.0132,\n",
      "        -0.8398, -0.3747, -2.2751, -0.9768])\n",
      "tensor([ 0.4048,  0.4500,  0.0289,  0.9127, -0.7483,  1.5466,  3.2040,  1.5057,\n",
      "        -0.4348, -0.0674, -0.3573, -1.6098])\n",
      "tensor([ 0.4379, -0.3244,  0.5542,  1.2276,  0.9054, -0.9495, -1.8944, -2.8664,\n",
      "        -0.4361,  0.2568,  0.0176, -1.0670])\n",
      "tensor([-1.0081, -0.1326,  0.0402,  0.2281,  0.3770, -0.1139, -2.2329,  0.4262,\n",
      "        -0.3503, -0.9822, -0.1224,  1.1088])\n",
      "tensor([ 0.4379, -0.3244,  0.5542,  1.2276,  0.9054, -0.9495, -1.8944, -2.8664,\n",
      "        -0.4361,  0.2568,  0.0176, -1.0670])\n",
      "tensor([ 1.0351,  0.3673,  0.0557, -0.8657, -1.0000, -0.8180, -0.3943, -1.0877,\n",
      "         0.4152, -1.1640, -1.2055,  0.9571])\n",
      "tensor([ 0.4379, -0.3244,  0.5542,  1.2276,  0.9054, -0.9495, -1.8944, -2.8664,\n",
      "        -0.4361,  0.2568,  0.0176, -1.0670])\n",
      "tensor([ 0.1241, -0.2803,  0.6302,  0.4358, -0.4561, -1.1573,  0.1540, -0.0535,\n",
      "         1.0204, -0.9439, -0.4562,  0.2757])\n",
      "tensor([ 0.4379, -0.3244,  0.5542,  1.2276,  0.9054, -0.9495, -1.8944, -2.8664,\n",
      "        -0.4361,  0.2568,  0.0176, -1.0670])\n",
      "tensor([-0.9350,  0.4967,  1.2768,  0.7883,  0.3773,  1.6894,  0.9493, -1.2904,\n",
      "         0.2382,  1.3618,  0.9784, -0.8932])\n",
      "tensor([ 0.4379, -0.3244,  0.5542,  1.2276,  0.9054, -0.9495, -1.8944, -2.8664,\n",
      "        -0.4361,  0.2568,  0.0176, -1.0670])\n",
      "tensor([-1.5437, -0.7408,  0.9241,  1.0720,  0.1766, -0.5238,  0.4340, -0.4353,\n",
      "        -1.1127,  1.4668, -0.9486, -0.6758])\n",
      "tensor([-0.2136, -1.7691, -0.1340, -2.4224, -2.3496, -0.7751, -0.2806, -1.0934,\n",
      "         0.0653, -0.3937, -0.1999, -1.3237])\n",
      "tensor([ 0.7179,  1.0248,  0.9483, -1.3332, -0.7922,  0.3112,  1.2247,  0.1719,\n",
      "        -1.2398,  1.1800, -1.6752,  0.2876])\n",
      "tensor([-0.2136, -1.7691, -0.1340, -2.4224, -2.3496, -0.7751, -0.2806, -1.0934,\n",
      "         0.0653, -0.3937, -0.1999, -1.3237])\n",
      "tensor([-0.9583, -0.1644,  1.3647, -1.8202, -0.5029, -0.2993, -0.1899,  0.4448,\n",
      "        -1.1722,  0.8239, -0.0912,  0.1398])\n",
      "tensor([-0.2136, -1.7691, -0.1340, -2.4224, -2.3496, -0.7751, -0.2806, -1.0934,\n",
      "         0.0653, -0.3937, -0.1999, -1.3237])\n",
      "tensor([ 1.0351,  0.3673,  0.0557, -0.8657, -1.0000, -0.8180, -0.3943, -1.0877,\n",
      "         0.4152, -1.1640, -1.2055,  0.9571])\n",
      "tensor([-0.2136, -1.7691, -0.1340, -2.4224, -2.3496, -0.7751, -0.2806, -1.0934,\n",
      "         0.0653, -0.3937, -0.1999, -1.3237])\n",
      "tensor([ 0.1241, -0.2803,  0.6302,  0.4358, -0.4561, -1.1573,  0.1540, -0.0535,\n",
      "         1.0204, -0.9439, -0.4562,  0.2757])\n",
      "tensor([-0.2136, -1.7691, -0.1340, -2.4224, -2.3496, -0.7751, -0.2806, -1.0934,\n",
      "         0.0653, -0.3937, -0.1999, -1.3237])\n",
      "tensor([-0.4894, -0.0323, -0.6528,  0.9902, -2.2690,  0.1405,  1.7084, -0.6363,\n",
      "         0.1503, -0.7970, -1.4187,  0.8197])\n",
      "tensor([-0.2136, -1.7691, -0.1340, -2.4224, -2.3496, -0.7751, -0.2806, -1.0934,\n",
      "         0.0653, -0.3937, -0.1999, -1.3237])\n",
      "tensor([-1.5437, -0.7408,  0.9241,  1.0720,  0.1766, -0.5238,  0.4340, -0.4353,\n",
      "        -1.1127,  1.4668, -0.9486, -0.6758])\n",
      "tensor([ 1.0941, -0.3783, -1.0907, -0.9276, -0.0155,  0.7337, -1.0442, -0.4161,\n",
      "        -0.1900, -1.7694,  1.2086,  0.3050])\n",
      "tensor([-1.0081, -0.1326,  0.0402,  0.2281,  0.3770, -0.1139, -2.2329,  0.4262,\n",
      "        -0.3503, -0.9822, -0.1224,  1.1088])\n",
      "tensor([ 1.0941, -0.3783, -1.0907, -0.9276, -0.0155,  0.7337, -1.0442, -0.4161,\n",
      "        -0.1900, -1.7694,  1.2086,  0.3050])\n",
      "tensor([ 1.0351,  0.3673,  0.0557, -0.8657, -1.0000, -0.8180, -0.3943, -1.0877,\n",
      "         0.4152, -1.1640, -1.2055,  0.9571])\n",
      "tensor([-0.5401, -0.8378, -0.2141,  0.8432,  1.2223,  0.5846, -0.4716, -2.1419,\n",
      "        -1.2016,  0.3841,  0.4306,  1.1719])\n",
      "tensor([-1.0081, -0.1326,  0.0402,  0.2281,  0.3770, -0.1139, -2.2329,  0.4262,\n",
      "        -0.3503, -0.9822, -0.1224,  1.1088])\n",
      "tensor([-0.5401, -0.8378, -0.2141,  0.8432,  1.2223,  0.5846, -0.4716, -2.1419,\n",
      "        -1.2016,  0.3841,  0.4306,  1.1719])\n",
      "tensor([-0.4894, -0.0323, -0.6528,  0.9902, -2.2690,  0.1405,  1.7084, -0.6363,\n",
      "         0.1503, -0.7970, -1.4187,  0.8197])\n",
      "tensor([-0.5401, -0.8378, -0.2141,  0.8432,  1.2223,  0.5846, -0.4716, -2.1419,\n",
      "        -1.2016,  0.3841,  0.4306,  1.1719])\n",
      "tensor([-0.9350,  0.4967,  1.2768,  0.7883,  0.3773,  1.6894,  0.9493, -1.2904,\n",
      "         0.2382,  1.3618,  0.9784, -0.8932])\n",
      "tensor([-0.5401, -0.8378, -0.2141,  0.8432,  1.2223,  0.5846, -0.4716, -2.1419,\n",
      "        -1.2016,  0.3841,  0.4306,  1.1719])\n",
      "tensor([-1.5437, -0.7408,  0.9241,  1.0720,  0.1766, -0.5238,  0.4340, -0.4353,\n",
      "        -1.1127,  1.4668, -0.9486, -0.6758])\n",
      "tensor([-2.2226,  0.0128,  0.4200,  0.9136,  1.8746,  0.8257,  0.6313, -0.5923,\n",
      "        -0.1127,  1.2902,  1.5221,  1.3186])\n",
      "tensor([-1.0081, -0.1326,  0.0402,  0.2281,  0.3770, -0.1139, -2.2329,  0.4262,\n",
      "        -0.3503, -0.9822, -0.1224,  1.1088])\n",
      "tensor([-2.2226,  0.0128,  0.4200,  0.9136,  1.8746,  0.8257,  0.6313, -0.5923,\n",
      "        -0.1127,  1.2902,  1.5221,  1.3186])\n",
      "tensor([-0.9583, -0.1644,  1.3647, -1.8202, -0.5029, -0.2993, -0.1899,  0.4448,\n",
      "        -1.1722,  0.8239, -0.0912,  0.1398])\n",
      "tensor([-2.2226,  0.0128,  0.4200,  0.9136,  1.8746,  0.8257,  0.6313, -0.5923,\n",
      "        -0.1127,  1.2902,  1.5221,  1.3186])\n",
      "tensor([-0.4394,  1.6873,  0.9231,  0.3819, -1.5698,  1.2345,  0.7134, -0.4158,\n",
      "         0.6749,  1.1299,  0.3694, -0.1149])\n",
      "tensor([-2.2226,  0.0128,  0.4200,  0.9136,  1.8746,  0.8257,  0.6313, -0.5923,\n",
      "        -0.1127,  1.2902,  1.5221,  1.3186])\n",
      "tensor([-0.9350,  0.4967,  1.2768,  0.7883,  0.3773,  1.6894,  0.9493, -1.2904,\n",
      "         0.2382,  1.3618,  0.9784, -0.8932])\n",
      "tensor([-2.2226,  0.0128,  0.4200,  0.9136,  1.8746,  0.8257,  0.6313, -0.5923,\n",
      "        -0.1127,  1.2902,  1.5221,  1.3186])\n",
      "tensor([-1.5437, -0.7408,  0.9241,  1.0720,  0.1766, -0.5238,  0.4340, -0.4353,\n",
      "        -1.1127,  1.4668, -0.9486, -0.6758])\n",
      "tensor([-2.1961,  0.1844, -0.6334,  0.2220, -0.0256, -2.7540,  1.4591,  1.7793,\n",
      "        -0.2333, -0.2885,  0.4468,  0.9863])\n",
      "tensor([-1.0081, -0.1326,  0.0402,  0.2281,  0.3770, -0.1139, -2.2329,  0.4262,\n",
      "        -0.3503, -0.9822, -0.1224,  1.1088])\n",
      "tensor([-2.1961,  0.1844, -0.6334,  0.2220, -0.0256, -2.7540,  1.4591,  1.7793,\n",
      "        -0.2333, -0.2885,  0.4468,  0.9863])\n",
      "tensor([-0.9583, -0.1644,  1.3647, -1.8202, -0.5029, -0.2993, -0.1899,  0.4448,\n",
      "        -1.1722,  0.8239, -0.0912,  0.1398])\n",
      "tensor([-2.1961,  0.1844, -0.6334,  0.2220, -0.0256, -2.7540,  1.4591,  1.7793,\n",
      "        -0.2333, -0.2885,  0.4468,  0.9863])\n",
      "tensor([ 0.1241, -0.2803,  0.6302,  0.4358, -0.4561, -1.1573,  0.1540, -0.0535,\n",
      "         1.0204, -0.9439, -0.4562,  0.2757])\n",
      "tensor([-2.1961,  0.1844, -0.6334,  0.2220, -0.0256, -2.7540,  1.4591,  1.7793,\n",
      "        -0.2333, -0.2885,  0.4468,  0.9863])\n",
      "tensor([-0.4894, -0.0323, -0.6528,  0.9902, -2.2690,  0.1405,  1.7084, -0.6363,\n",
      "         0.1503, -0.7970, -1.4187,  0.8197])\n",
      "tensor([-2.1961,  0.1844, -0.6334,  0.2220, -0.0256, -2.7540,  1.4591,  1.7793,\n",
      "        -0.2333, -0.2885,  0.4468,  0.9863])\n",
      "tensor([-1.5437, -0.7408,  0.9241,  1.0720,  0.1766, -0.5238,  0.4340, -0.4353,\n",
      "        -1.1127,  1.4668, -0.9486, -0.6758])\n",
      "tensor([-2.1961,  0.1844, -0.6334,  0.2220, -0.0256, -2.7540,  1.4591,  1.7793,\n",
      "        -0.2333, -0.2885,  0.4468,  0.9863])\n",
      "tensor([ 0.4048,  0.4500,  0.0289,  0.9127, -0.7483,  1.5466,  3.2040,  1.5057,\n",
      "        -0.4348, -0.0674, -0.3573, -1.6098])\n",
      "tensor([ 1.8294, -0.2486, -0.0582,  0.2816,  0.6279, -1.8369, -0.3991, -1.0778,\n",
      "        -0.8298,  0.8910,  0.5986,  0.3604])\n",
      "tensor([ 0.3822, -0.0656,  1.3687, -0.5633,  1.3572, -0.0758,  0.6805, -0.9006,\n",
      "         1.0606,  1.9753,  0.9683, -0.7341])\n",
      "tensor([ 1.8294, -0.2486, -0.0582,  0.2816,  0.6279, -1.8369, -0.3991, -1.0778,\n",
      "        -0.8298,  0.8910,  0.5986,  0.3604])\n",
      "tensor([ 1.3935, -0.0956,  1.4219,  0.7959,  2.0271, -0.3702, -0.0987, -1.8875,\n",
      "        -0.8734, -1.0314, -0.5359,  0.6895])\n",
      "tensor([ 1.8294, -0.2486, -0.0582,  0.2816,  0.6279, -1.8369, -0.3991, -1.0778,\n",
      "        -0.8298,  0.8910,  0.5986,  0.3604])\n",
      "tensor([ 0.7919, -0.5167, -0.4782, -0.6079, -0.7759, -0.8913, -0.2027, -1.2061,\n",
      "         0.3037,  0.7620,  0.3392, -0.4011])\n",
      "tensor([ 0.5227, -1.3420,  2.2447, -0.3020,  1.0325,  0.6256, -0.6325,  0.8273,\n",
      "         0.9681, -1.2471, -0.6062,  1.0958])\n",
      "tensor([ 0.3822, -0.0656,  1.3687, -0.5633,  1.3572, -0.0758,  0.6805, -0.9006,\n",
      "         1.0606,  1.9753,  0.9683, -0.7341])\n",
      "tensor([ 0.5227, -1.3420,  2.2447, -0.3020,  1.0325,  0.6256, -0.6325,  0.8273,\n",
      "         0.9681, -1.2471, -0.6062,  1.0958])\n",
      "tensor([-1.1890,  0.2648,  0.3921, -0.1455,  1.0312,  1.7158,  0.5537,  0.8058,\n",
      "        -1.6310, -1.2671,  1.3806,  0.7165])\n",
      "tensor([ 0.5227, -1.3420,  2.2447, -0.3020,  1.0325,  0.6256, -0.6325,  0.8273,\n",
      "         0.9681, -1.2471, -0.6062,  1.0958])\n",
      "tensor([ 0.1098,  3.6705,  1.5268, -1.2056, -0.9185,  1.3819, -0.1878, -0.6243,\n",
      "         1.0824, -1.9486, -0.9777, -0.2814])\n",
      "tensor([ 0.5227, -1.3420,  2.2447, -0.3020,  1.0325,  0.6256, -0.6325,  0.8273,\n",
      "         0.9681, -1.2471, -0.6062,  1.0958])\n",
      "tensor([ 1.3935, -0.0956,  1.4219,  0.7959,  2.0271, -0.3702, -0.0987, -1.8875,\n",
      "        -0.8734, -1.0314, -0.5359,  0.6895])\n",
      "tensor([ 0.5227, -1.3420,  2.2447, -0.3020,  1.0325,  0.6256, -0.6325,  0.8273,\n",
      "         0.9681, -1.2471, -0.6062,  1.0958])\n",
      "tensor([-0.8321,  0.6150,  1.1488,  1.0360,  1.1475,  1.2083,  0.9555, -0.5605,\n",
      "        -1.0423, -1.6032,  0.0723,  1.0594])\n",
      "tensor([ 0.5227, -1.3420,  2.2447, -0.3020,  1.0325,  0.6256, -0.6325,  0.8273,\n",
      "         0.9681, -1.2471, -0.6062,  1.0958])\n",
      "tensor([-0.9463,  0.1723, -0.6194, -1.9738,  1.6902,  1.0256,  1.8230,  0.7055,\n",
      "        -0.1371,  1.4551, -1.6595,  1.5422])\n",
      "tensor([ 0.5227, -1.3420,  2.2447, -0.3020,  1.0325,  0.6256, -0.6325,  0.8273,\n",
      "         0.9681, -1.2471, -0.6062,  1.0958])\n",
      "tensor([ 1.0406, -0.6422,  0.2393, -0.1238,  0.1912,  0.8894,  1.4560,  1.6830,\n",
      "        -0.0502, -0.3793, -0.0771, -0.6209])\n",
      "tensor([-0.8949,  0.3907, -1.8062, -1.1877,  0.6547,  0.3726, -0.1605, -0.5840,\n",
      "        -0.1025,  0.2678, -1.7196,  0.5341])\n",
      "tensor([-0.5081,  0.1018, -1.1813,  0.3674, -2.0492,  1.1363, -0.3405, -0.2442,\n",
      "        -0.1206,  1.4382,  0.9180,  0.3879])\n",
      "tensor([-0.8949,  0.3907, -1.8062, -1.1877,  0.6547,  0.3726, -0.1605, -0.5840,\n",
      "        -0.1025,  0.2678, -1.7196,  0.5341])\n",
      "tensor([ 0.1098,  3.6705,  1.5268, -1.2056, -0.9185,  1.3819, -0.1878, -0.6243,\n",
      "         1.0824, -1.9486, -0.9777, -0.2814])\n",
      "tensor([-0.8949,  0.3907, -1.8062, -1.1877,  0.6547,  0.3726, -0.1605, -0.5840,\n",
      "        -0.1025,  0.2678, -1.7196,  0.5341])\n",
      "tensor([-0.9463,  0.1723, -0.6194, -1.9738,  1.6902,  1.0256,  1.8230,  0.7055,\n",
      "        -0.1371,  1.4551, -1.6595,  1.5422])\n",
      "tensor([-0.8949,  0.3907, -1.8062, -1.1877,  0.6547,  0.3726, -0.1605, -0.5840,\n",
      "        -0.1025,  0.2678, -1.7196,  0.5341])\n",
      "tensor([-0.8278, -2.3965, -0.8038,  0.3332,  0.3631,  0.2880,  1.0687,  0.5172,\n",
      "        -0.7898,  0.8170,  0.0696,  0.1665])\n",
      "tensor([-6.2452e-01,  1.0242e+00,  6.9277e-01, -1.6932e-01, -4.8196e-01,\n",
      "         1.1099e+00, -3.9495e-04, -9.6511e-02, -1.5466e-01, -1.0715e+00,\n",
      "         4.5606e-01,  1.0657e+00])\n",
      "tensor([-1.1890,  0.2648,  0.3921, -0.1455,  1.0312,  1.7158,  0.5537,  0.8058,\n",
      "        -1.6310, -1.2671,  1.3806,  0.7165])\n",
      "tensor([-6.2452e-01,  1.0242e+00,  6.9277e-01, -1.6932e-01, -4.8196e-01,\n",
      "         1.1099e+00, -3.9495e-04, -9.6511e-02, -1.5466e-01, -1.0715e+00,\n",
      "         4.5606e-01,  1.0657e+00])\n",
      "tensor([-0.5081,  0.1018, -1.1813,  0.3674, -2.0492,  1.1363, -0.3405, -0.2442,\n",
      "        -0.1206,  1.4382,  0.9180,  0.3879])\n",
      "tensor([-6.2452e-01,  1.0242e+00,  6.9277e-01, -1.6932e-01, -4.8196e-01,\n",
      "         1.1099e+00, -3.9495e-04, -9.6511e-02, -1.5466e-01, -1.0715e+00,\n",
      "         4.5606e-01,  1.0657e+00])\n",
      "tensor([ 0.1098,  3.6705,  1.5268, -1.2056, -0.9185,  1.3819, -0.1878, -0.6243,\n",
      "         1.0824, -1.9486, -0.9777, -0.2814])\n",
      "tensor([-6.2452e-01,  1.0242e+00,  6.9277e-01, -1.6932e-01, -4.8196e-01,\n",
      "         1.1099e+00, -3.9495e-04, -9.6511e-02, -1.5466e-01, -1.0715e+00,\n",
      "         4.5606e-01,  1.0657e+00])\n",
      "tensor([ 1.3935, -0.0956,  1.4219,  0.7959,  2.0271, -0.3702, -0.0987, -1.8875,\n",
      "        -0.8734, -1.0314, -0.5359,  0.6895])\n",
      "tensor([-6.2452e-01,  1.0242e+00,  6.9277e-01, -1.6932e-01, -4.8196e-01,\n",
      "         1.1099e+00, -3.9495e-04, -9.6511e-02, -1.5466e-01, -1.0715e+00,\n",
      "         4.5606e-01,  1.0657e+00])\n",
      "tensor([-0.8321,  0.6150,  1.1488,  1.0360,  1.1475,  1.2083,  0.9555, -0.5605,\n",
      "        -1.0423, -1.6032,  0.0723,  1.0594])\n",
      "tensor([-6.2452e-01,  1.0242e+00,  6.9277e-01, -1.6932e-01, -4.8196e-01,\n",
      "         1.1099e+00, -3.9495e-04, -9.6511e-02, -1.5466e-01, -1.0715e+00,\n",
      "         4.5606e-01,  1.0657e+00])\n",
      "tensor([-0.9463,  0.1723, -0.6194, -1.9738,  1.6902,  1.0256,  1.8230,  0.7055,\n",
      "        -0.1371,  1.4551, -1.6595,  1.5422])\n",
      "tensor([-1.5533, -0.2206, -1.0813,  0.6792, -0.7854,  0.3672, -0.7930,  0.3459,\n",
      "        -0.3486,  1.3293,  2.1011, -0.0271])\n",
      "tensor([-1.1890,  0.2648,  0.3921, -0.1455,  1.0312,  1.7158,  0.5537,  0.8058,\n",
      "        -1.6310, -1.2671,  1.3806,  0.7165])\n",
      "tensor([-1.5533, -0.2206, -1.0813,  0.6792, -0.7854,  0.3672, -0.7930,  0.3459,\n",
      "        -0.3486,  1.3293,  2.1011, -0.0271])\n",
      "tensor([-0.5081,  0.1018, -1.1813,  0.3674, -2.0492,  1.1363, -0.3405, -0.2442,\n",
      "        -0.1206,  1.4382,  0.9180,  0.3879])\n",
      "tensor([-1.5533, -0.2206, -1.0813,  0.6792, -0.7854,  0.3672, -0.7930,  0.3459,\n",
      "        -0.3486,  1.3293,  2.1011, -0.0271])\n",
      "tensor([-0.8278, -2.3965, -0.8038,  0.3332,  0.3631,  0.2880,  1.0687,  0.5172,\n",
      "        -0.7898,  0.8170,  0.0696,  0.1665])\n",
      "tensor([-1.5533, -0.2206, -1.0813,  0.6792, -0.7854,  0.3672, -0.7930,  0.3459,\n",
      "        -0.3486,  1.3293,  2.1011, -0.0271])\n",
      "tensor([ 0.7919, -0.5167, -0.4782, -0.6079, -0.7759, -0.8913, -0.2027, -1.2061,\n",
      "         0.3037,  0.7620,  0.3392, -0.4011])\n",
      "tensor([ 0.2015,  0.6814,  0.1778,  0.5670,  0.7267, -0.8623, -0.9636, -1.1081,\n",
      "        -1.7791,  0.8192, -0.1706, -0.8296])\n",
      "tensor([ 0.3822, -0.0656,  1.3687, -0.5633,  1.3572, -0.0758,  0.6805, -0.9006,\n",
      "         1.0606,  1.9753,  0.9683, -0.7341])\n",
      "tensor([ 0.2015,  0.6814,  0.1778,  0.5670,  0.7267, -0.8623, -0.9636, -1.1081,\n",
      "        -1.7791,  0.8192, -0.1706, -0.8296])\n",
      "tensor([ 1.3935, -0.0956,  1.4219,  0.7959,  2.0271, -0.3702, -0.0987, -1.8875,\n",
      "        -0.8734, -1.0314, -0.5359,  0.6895])\n",
      "tensor([ 0.2015,  0.6814,  0.1778,  0.5670,  0.7267, -0.8623, -0.9636, -1.1081,\n",
      "        -1.7791,  0.8192, -0.1706, -0.8296])\n",
      "tensor([-0.8321,  0.6150,  1.1488,  1.0360,  1.1475,  1.2083,  0.9555, -0.5605,\n",
      "        -1.0423, -1.6032,  0.0723,  1.0594])\n",
      "tensor([ 0.2015,  0.6814,  0.1778,  0.5670,  0.7267, -0.8623, -0.9636, -1.1081,\n",
      "        -1.7791,  0.8192, -0.1706, -0.8296])\n",
      "tensor([ 0.7919, -0.5167, -0.4782, -0.6079, -0.7759, -0.8913, -0.2027, -1.2061,\n",
      "         0.3037,  0.7620,  0.3392, -0.4011])\n",
      "tensor([ 0.8544, -0.5190, -1.2991, -0.2259, -0.1131, -0.3625,  0.5519,  0.0213,\n",
      "         0.4780,  1.4259,  0.1132, -0.4039])\n",
      "tensor([ 0.3822, -0.0656,  1.3687, -0.5633,  1.3572, -0.0758,  0.6805, -0.9006,\n",
      "         1.0606,  1.9753,  0.9683, -0.7341])\n",
      "tensor([ 0.8544, -0.5190, -1.2991, -0.2259, -0.1131, -0.3625,  0.5519,  0.0213,\n",
      "         0.4780,  1.4259,  0.1132, -0.4039])\n",
      "tensor([-0.5081,  0.1018, -1.1813,  0.3674, -2.0492,  1.1363, -0.3405, -0.2442,\n",
      "        -0.1206,  1.4382,  0.9180,  0.3879])\n",
      "tensor([ 0.8544, -0.5190, -1.2991, -0.2259, -0.1131, -0.3625,  0.5519,  0.0213,\n",
      "         0.4780,  1.4259,  0.1132, -0.4039])\n",
      "tensor([-0.9463,  0.1723, -0.6194, -1.9738,  1.6902,  1.0256,  1.8230,  0.7055,\n",
      "        -0.1371,  1.4551, -1.6595,  1.5422])\n",
      "tensor([ 0.8544, -0.5190, -1.2991, -0.2259, -0.1131, -0.3625,  0.5519,  0.0213,\n",
      "         0.4780,  1.4259,  0.1132, -0.4039])\n",
      "tensor([ 1.0406, -0.6422,  0.2393, -0.1238,  0.1912,  0.8894,  1.4560,  1.6830,\n",
      "        -0.0502, -0.3793, -0.0771, -0.6209])\n",
      "tensor([ 0.8544, -0.5190, -1.2991, -0.2259, -0.1131, -0.3625,  0.5519,  0.0213,\n",
      "         0.4780,  1.4259,  0.1132, -0.4039])\n",
      "tensor([-0.8278, -2.3965, -0.8038,  0.3332,  0.3631,  0.2880,  1.0687,  0.5172,\n",
      "        -0.7898,  0.8170,  0.0696,  0.1665])\n",
      "tensor([ 0.8544, -0.5190, -1.2991, -0.2259, -0.1131, -0.3625,  0.5519,  0.0213,\n",
      "         0.4780,  1.4259,  0.1132, -0.4039])\n",
      "tensor([ 0.7919, -0.5167, -0.4782, -0.6079, -0.7759, -0.8913, -0.2027, -1.2061,\n",
      "         0.3037,  0.7620,  0.3392, -0.4011])\n",
      "tensor([-0.7046,  0.2941,  1.0703, -0.6493, -0.0858, -0.8090, -0.6463,  0.9225,\n",
      "         2.0126,  0.8597,  0.2666, -0.0033])\n",
      "tensor([ 0.3822, -0.0656,  1.3687, -0.5633,  1.3572, -0.0758,  0.6805, -0.9006,\n",
      "         1.0606,  1.9753,  0.9683, -0.7341])\n",
      "tensor([-0.7046,  0.2941,  1.0703, -0.6493, -0.0858, -0.8090, -0.6463,  0.9225,\n",
      "         2.0126,  0.8597,  0.2666, -0.0033])\n",
      "tensor([ 0.1098,  3.6705,  1.5268, -1.2056, -0.9185,  1.3819, -0.1878, -0.6243,\n",
      "         1.0824, -1.9486, -0.9777, -0.2814])\n",
      "tensor([-0.7046,  0.2941,  1.0703, -0.6493, -0.0858, -0.8090, -0.6463,  0.9225,\n",
      "         2.0126,  0.8597,  0.2666, -0.0033])\n",
      "tensor([-0.9463,  0.1723, -0.6194, -1.9738,  1.6902,  1.0256,  1.8230,  0.7055,\n",
      "        -0.1371,  1.4551, -1.6595,  1.5422])\n",
      "tensor([-0.7046,  0.2941,  1.0703, -0.6493, -0.0858, -0.8090, -0.6463,  0.9225,\n",
      "         2.0126,  0.8597,  0.2666, -0.0033])\n",
      "tensor([ 0.7919, -0.5167, -0.4782, -0.6079, -0.7759, -0.8913, -0.2027, -1.2061,\n",
      "         0.3037,  0.7620,  0.3392, -0.4011])\n",
      "tensor([-0.6404, -1.0726,  0.5936, -1.1981,  0.3963, -0.6191,  0.2222, -0.3994,\n",
      "        -0.2431, -0.0912, -0.8552,  0.9193])\n",
      "tensor([ 0.3822, -0.0656,  1.3687, -0.5633,  1.3572, -0.0758,  0.6805, -0.9006,\n",
      "         1.0606,  1.9753,  0.9683, -0.7341])\n",
      "tensor([-0.6404, -1.0726,  0.5936, -1.1981,  0.3963, -0.6191,  0.2222, -0.3994,\n",
      "        -0.2431, -0.0912, -0.8552,  0.9193])\n",
      "tensor([-1.1890,  0.2648,  0.3921, -0.1455,  1.0312,  1.7158,  0.5537,  0.8058,\n",
      "        -1.6310, -1.2671,  1.3806,  0.7165])\n",
      "tensor([-0.6404, -1.0726,  0.5936, -1.1981,  0.3963, -0.6191,  0.2222, -0.3994,\n",
      "        -0.2431, -0.0912, -0.8552,  0.9193])\n",
      "tensor([ 1.3935, -0.0956,  1.4219,  0.7959,  2.0271, -0.3702, -0.0987, -1.8875,\n",
      "        -0.8734, -1.0314, -0.5359,  0.6895])\n",
      "tensor([-0.6404, -1.0726,  0.5936, -1.1981,  0.3963, -0.6191,  0.2222, -0.3994,\n",
      "        -0.2431, -0.0912, -0.8552,  0.9193])\n",
      "tensor([-0.8321,  0.6150,  1.1488,  1.0360,  1.1475,  1.2083,  0.9555, -0.5605,\n",
      "        -1.0423, -1.6032,  0.0723,  1.0594])\n",
      "tensor([-0.6404, -1.0726,  0.5936, -1.1981,  0.3963, -0.6191,  0.2222, -0.3994,\n",
      "        -0.2431, -0.0912, -0.8552,  0.9193])\n",
      "tensor([-0.9463,  0.1723, -0.6194, -1.9738,  1.6902,  1.0256,  1.8230,  0.7055,\n",
      "        -0.1371,  1.4551, -1.6595,  1.5422])\n",
      "tensor([-0.6404, -1.0726,  0.5936, -1.1981,  0.3963, -0.6191,  0.2222, -0.3994,\n",
      "        -0.2431, -0.0912, -0.8552,  0.9193])\n",
      "tensor([-0.8278, -2.3965, -0.8038,  0.3332,  0.3631,  0.2880,  1.0687,  0.5172,\n",
      "        -0.7898,  0.8170,  0.0696,  0.1665])\n",
      "tensor([-0.6404, -1.0726,  0.5936, -1.1981,  0.3963, -0.6191,  0.2222, -0.3994,\n",
      "        -0.2431, -0.0912, -0.8552,  0.9193])\n",
      "tensor([ 0.7919, -0.5167, -0.4782, -0.6079, -0.7759, -0.8913, -0.2027, -1.2061,\n",
      "         0.3037,  0.7620,  0.3392, -0.4011])\n",
      "tensor([ 1.1070,  1.5228,  0.3220, -3.3829, -1.5674, -2.4538,  0.4082, -0.4938,\n",
      "        -1.5992,  0.9629,  0.4411,  1.2942])\n",
      "tensor([ 0.3822, -0.0656,  1.3687, -0.5633,  1.3572, -0.0758,  0.6805, -0.9006,\n",
      "         1.0606,  1.9753,  0.9683, -0.7341])\n",
      "tensor([ 1.1070,  1.5228,  0.3220, -3.3829, -1.5674, -2.4538,  0.4082, -0.4938,\n",
      "        -1.5992,  0.9629,  0.4411,  1.2942])\n",
      "tensor([-0.5081,  0.1018, -1.1813,  0.3674, -2.0492,  1.1363, -0.3405, -0.2442,\n",
      "        -0.1206,  1.4382,  0.9180,  0.3879])\n",
      "tensor([ 1.1070,  1.5228,  0.3220, -3.3829, -1.5674, -2.4538,  0.4082, -0.4938,\n",
      "        -1.5992,  0.9629,  0.4411,  1.2942])\n",
      "tensor([ 0.1098,  3.6705,  1.5268, -1.2056, -0.9185,  1.3819, -0.1878, -0.6243,\n",
      "         1.0824, -1.9486, -0.9777, -0.2814])\n",
      "tensor([ 1.1070,  1.5228,  0.3220, -3.3829, -1.5674, -2.4538,  0.4082, -0.4938,\n",
      "        -1.5992,  0.9629,  0.4411,  1.2942])\n",
      "tensor([-0.9463,  0.1723, -0.6194, -1.9738,  1.6902,  1.0256,  1.8230,  0.7055,\n",
      "        -0.1371,  1.4551, -1.6595,  1.5422])\n",
      "tensor([ 1.1070,  1.5228,  0.3220, -3.3829, -1.5674, -2.4538,  0.4082, -0.4938,\n",
      "        -1.5992,  0.9629,  0.4411,  1.2942])\n",
      "tensor([ 0.7919, -0.5167, -0.4782, -0.6079, -0.7759, -0.8913, -0.2027, -1.2061,\n",
      "         0.3037,  0.7620,  0.3392, -0.4011])\n"
     ]
    }
   ],
   "source": [
    "for i in multimodal_mutual_info:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  2, 15, 23, 54,  6, 51, 25, 58, 13, 43, 16, 45,  1, 42, 37, 52, 33,\n",
      "        49, 59, 50, 36, 11,  8, 29,  4, 27, 35, 32])\n",
      "tensor([40, 14, 15, 37, 45, 11, 43,  2, 41, 50,  5,  1, 58, 52, 22,  4,  6, 21,\n",
      "        36,  7, 38, 32, 47,  9, 18, 26,  3, 53, 49])\n",
      "29\n",
      "tensor([ 5,  2, 15, 23, 54,  6, 51, 25, 58, 13, 43, 16, 45,  1, 42, 37, 52, 33,\n",
      "        49, 59, 50, 36, 11,  8, 29,  4, 27, 35, 32])\n",
      "tensor([18, 27, 21, 31,  7, 56, 24, 25, 15, 17, 52, 50, 45,  9,  0, 42, 47, 11,\n",
      "        34, 26, 32, 54, 35, 30, 58, 48, 10, 19, 13])\n",
      "tensor([28, 15, 19,  3, 59, 54, 32, 53, 52, 51, 13, 47,  9, 17, 33, 49, 41, 38,\n",
      "        44,  1, 16, 45,  2,  4, 56,  6, 58, 26, 14])\n",
      "29\n",
      "tensor([18, 27, 21, 31,  7, 56, 24, 25, 15, 17, 52, 50, 45,  9,  0, 42, 47, 11,\n",
      "        34, 26, 32, 54, 35, 30, 58, 48, 10, 19, 13])\n",
      "torch.Size([2, 29, 512])\n",
      "torch.Size([2, 29, 512])\n",
      "torch.Size([2, 31, 512])\n",
      "torch.Size([2, 31, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "sequence_length = 60  # 序列长度\n",
    "batch_size = 2  # 批量大小\n",
    "hidden_size = 512  # 特征大小\n",
    "def split_feature(video_embeddings, text_embeddings, batch_size, hidden_size):\n",
    "    # 确保视频和文本嵌入的batch size是相同的\n",
    "    assert video_embeddings.size(0) == text_embeddings.size(0)\n",
    "    \n",
    "    # 获取嵌入维度\n",
    "    v_b, v_l, v_h = video_embeddings.shape\n",
    "    t_b, t_l, t_h = text_embeddings.shape\n",
    "    assert v_h == t_h  # 确保视频和文本嵌入在特征维度上是相同的\n",
    "\n",
    "    # 初始化列表来存储共有信息和各自模态的独有信息\n",
    "\n",
    "    video_mutual_info = [[] for i in range(batch_size)]\n",
    "    text_mutual_info = [[] for i in range(batch_size)]\n",
    "    video_only_info = [[] for i in range(batch_size)]\n",
    "    text_only_info = [[] for i in range(batch_size)]\n",
    "\n",
    "    # 对每个batch进行处理\n",
    "    for batch_idx in range(v_b):\n",
    "        # 计算当前batch中所有特征对的相似度\n",
    "        similarities = torch.zeros((v_l, t_l))\n",
    "        for vl in range(v_l):\n",
    "            for tl in range(t_l):\n",
    "                # 计算视频特征和文本特征之间的余弦相似度\n",
    "                cos_sim = F.cosine_similarity(video_embeddings[batch_idx][vl].unsqueeze(0), text_embeddings[batch_idx][tl].unsqueeze(0))\n",
    "                similarities[vl, tl] = cos_sim\n",
    "\n",
    "        # 将相似度矩阵转换为一维数组并排序，选择相似度最高的50%\n",
    "        flat_similarities = similarities.view(-1)\n",
    "        sorted_similarities, sorted_indices = torch.sort(flat_similarities, descending=True)\n",
    "        top_half_indices = sorted_indices[:len(sorted_indices) // 2]\n",
    "\n",
    "\n",
    "\n",
    "        # 选取前50%的相似度对应的特征\n",
    "        selected_video_indices = top_half_indices // t_l\n",
    "        selected_text_indices = top_half_indices % t_l\n",
    "\n",
    "\n",
    "        selected_video_indices=torch.unique(selected_video_indices,sorted=False)[:vl//2]\n",
    "\n",
    "        selected_text_indices=torch.unique(selected_text_indices,sorted=False)[:tl//2]\n",
    "        print(selected_text_indices)\n",
    "        print(selected_video_indices)\n",
    "\n",
    "        for vi in range(v_l):\n",
    "            if vi in selected_video_indices:\n",
    "                video_mutual_info[batch_idx].append(video_embeddings[batch_idx, vi])\n",
    "            else:\n",
    "                video_only_info[batch_idx].append(video_embeddings[batch_idx, vi])\n",
    "\n",
    "\n",
    "        count=0\n",
    "        for ti in range(t_l):\n",
    "            if ti in selected_text_indices:\n",
    "                # print(ti)\n",
    "                count+=1\n",
    "                text_mutual_info[batch_idx].append(text_embeddings[batch_idx, ti])\n",
    "\n",
    "            else:\n",
    "                text_only_info[batch_idx].append(text_embeddings[batch_idx, ti])\n",
    "        print(count)\n",
    "        print(selected_text_indices)\n",
    "\n",
    "        \n",
    "    max_length = max(len(sublist) for sublist in video_mutual_info)\n",
    "    for batch_id in range(batch_size):\n",
    "        if len(video_mutual_info[batch_id])<max_length:\n",
    "            for i in range(len(video_mutual_info[batch_id]),max_length):\n",
    "                video_mutual_info[batch_id].append(torch.zeros([hidden_size]))\n",
    "\n",
    "    max_length = max(len(sublist) for sublist in text_mutual_info)\n",
    "    for batch_id in range(batch_size):\n",
    "        if len(text_mutual_info[batch_id])<max_length:\n",
    "            for i in range(len(text_mutual_info[batch_id]),max_length):\n",
    "                text_mutual_info[batch_id].append(torch.zeros([hidden_size]))\n",
    "\n",
    "    max_length = max(len(sublist) for sublist in text_only_info)\n",
    "    for batch_id in range(batch_size):\n",
    "        if len(text_only_info[batch_id])<max_length:\n",
    "            for i in range(len(text_only_info[batch_id]),max_length):\n",
    "                text_only_info[batch_id].append(torch.zeros([hidden_size]))\n",
    "\n",
    "        \n",
    "    max_length = max(len(sublist) for sublist in video_only_info)\n",
    "    for batch_id in range(batch_size):\n",
    "        if len(video_only_info[batch_id])<max_length:\n",
    "            for i in range(len(video_only_info[batch_id]),max_length):\n",
    "                video_only_info[batch_id].append(torch.zeros([hidden_size]))\n",
    "\n",
    "    video_mutual_info = torch.stack([torch.stack(sublist) for sublist in video_mutual_info])\n",
    "    text_mutual_info = torch.stack([torch.stack(sublist) for sublist in text_mutual_info])\n",
    "    text_only_info = torch.stack([torch.stack(sublist) for sublist in text_only_info])\n",
    "    video_only_info = torch.stack([torch.stack(sublist) for sublist in video_only_info])\n",
    "\n",
    "\n",
    "    return video_mutual_info, text_mutual_info, video_only_info, text_only_info\n",
    "\n",
    "# 假设的视频和文本嵌入向量\n",
    "video_embeddings = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "text_embeddings = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# 调用函数\n",
    "video_mutual_info, text_mutual_info, video_only_info, text_only_info = split_feature(video_embeddings, text_embeddings,batch_size, hidden_size)\n",
    "\n",
    "# 检查结果\n",
    "print(video_mutual_info.shape)\n",
    "print(text_mutual_info.shape)\n",
    "print(video_only_info.shape)\n",
    "print(text_only_info.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 58, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 31, 512])\n",
      "torch.Size([2, 31, 1024])\n"
     ]
    }
   ],
   "source": [
    "multimodal_mutual_info = torch.cat((video_mutual_info, text_mutual_info),dim=1)\n",
    "print(multimodal_mutual_info.shape)\n",
    "multimodal_mutual_info=torch.mean(multimodal_mutual_info,dim=1)\n",
    "print(multimodal_mutual_info.shape)\n",
    "mutual_info_expanded = multimodal_mutual_info.unsqueeze(1).expand(-1, video_only_info.size(1), -1)\n",
    "print(mutual_info_expanded.shape)       \n",
    "video_only_info_gate=torch.cat((video_only_info,mutual_info_expanded),dim=2)\n",
    "print(video_only_info_gate.shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "受控制的视频信息形状: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GateControlledModel(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(GateControlledModel, self).__init__()\n",
    "        # 定义一个线性层\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)  # *2因为我们将连接两个隐藏层\n",
    "\n",
    "    def forward(self, video_info, mutual_info):\n",
    "        # video_info形状: [batch_size, sequence_length, hidden_size]\n",
    "        # mutual_info形状: [batch_size, hidden_size]\n",
    "        \n",
    "        # 将mutual_info扩展到与video_info的序列长度相匹配\n",
    "        mutual_info_expanded = mutual_info.unsqueeze(1).expand(-1, video_info.size(1), -1)\n",
    "        \n",
    "        # 将mutual_info和video_info在最后一个维度上连接\n",
    "        combined = torch.cat((video_info, mutual_info_expanded), dim=2)\n",
    "        \n",
    "        # 通过线性层\n",
    "        gate_signals = self.linear(combined)\n",
    "        \n",
    "        # 应用sigmoid激活函数以获取门控信号\n",
    "        gate_signals = torch.sigmoid(gate_signals)\n",
    "        \n",
    "        # 将门控信号应用到video_info上\n",
    "        controlled_video_info = video_info * gate_signals\n",
    "        \n",
    "        return controlled_video_info\n",
    "\n",
    "# 假设隐藏层大小和输出大小\n",
    "hidden_size = 128\n",
    "output_size = 128  # 输出的每个维度都需要一个门控信号\n",
    "\n",
    "# 创建模型实例\n",
    "model = GateControlledModel(hidden_size, output_size)\n",
    "\n",
    "# 假设的输入数据\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "video_info = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "mutual_info = torch.randn(batch_size, hidden_size)\n",
    "\n",
    "# 调用模型\n",
    "controlled_video_output = model(video_info, mutual_info)\n",
    "\n",
    "print(\"受控制的视频信息形状:\", controlled_video_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_info tensor([-0.4900, -0.5619, -1.1948,  0.6116,  0.3057,  0.6802, -2.3112,  1.4524])\n",
      "text_info tensor([ 1.3749,  0.7404, -0.4067,  0.5937,  0.7644, -0.2825,  0.1739,  0.2631])\n",
      "average_tensor tensor([ 0.4424,  0.0892, -0.8007,  0.6026,  0.5350,  0.1988, -1.0686,  0.8577])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "video_info = torch.randn(2, 5, 8)\n",
    "text_info = torch.randn(2, 5,8)\n",
    "# 假设 v2t_attn 和 t2v_attn 已经定义并且它们的形状都是 [2, 60, 512]\n",
    "# 首先计算两个张量的平均值\n",
    "print(\"video_info\",video_info[0][0])\n",
    "print(\"text_info\",text_info[0][0])\n",
    "average_tensor = (video_info + text_info) / 2\n",
    "print(\"average_tensor\",average_tensor[0][0])\n",
    "\n",
    "# # 接下来，如果需要在512维度上求均值，使用 torch.mean 并指定 dim=2\n",
    "# mean_along_512 = torch.mean(average_tensor, dim=2)\n",
    "\n",
    "# # mean_along_512 的形状现在是 [2, 60]，因为我们在每个 [2, 60, 512] 的张量上沿着最后一个维度求了均值\n",
    "# print(mean_along_512.shape)  # 应该打印出 [2, 60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator KMeans from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "video_info = torch.randn(2, 512)\n",
    "with open(\"kmeans_model_100.pkl\", \"rb\") as f:\n",
    "    k_means =  pkl.load(f)\n",
    "topic = k_means.transform(video_info.detach().cpu().numpy())\n",
    "print(topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected values: torch.Size([2, 3, 512])\n",
      "Selected indices: torch.Size([2, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gumbel_softmax(logits, temperature, k, eps=1e-10):\n",
    "    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + eps) + eps)\n",
    "    logits_with_noise = logits + gumbel_noise\n",
    "    softmax = torch.softmax(logits_with_noise / temperature, dim=-1)\n",
    "    values, indices = torch.topk(softmax, k, dim=1)\n",
    "    return values, indices\n",
    "\n",
    "# 示例：用随机数据测试 Gumbel-Top-k\n",
    "logits = torch.randn(2, 60,512)  # 假设有10个patches的logits\n",
    "temperature = 0.5  # 温度参数控制软化程度\n",
    "k = 3  # 选择Top-3的元素\n",
    "\n",
    "values, indices = gumbel_softmax(logits, temperature, k)\n",
    "print(\"Selected values:\", values.shape)\n",
    "print(\"Selected indices:\", indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(indices.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 128])\n",
      "torch.Size([1, 3, 128])\n",
      "tensor([[-0.0055,  0.1026,  0.0889],\n",
      "        [ 0.1055, -0.0059, -0.0020],\n",
      "        [-0.1103, -0.1274, -0.0508]])\n",
      "Contrastive Loss: 0.5302807688713074\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, text_features, image_features):\n",
    "        # 计算文本特征和图像特征之间的相似度\n",
    "        print(text_features.unsqueeze(1).shape)\n",
    "        print(image_features.unsqueeze(0).shape)\n",
    "        similarities = F.cosine_similarity(text_features.unsqueeze(1), image_features.unsqueeze(0), dim=2)\n",
    "        print(similarities)\n",
    "        \n",
    "        # 对角线元素是正样本对的相似度\n",
    "        positive_similarities = similarities.diag()\n",
    "\n",
    "        # 计算每个正样本对与所有负样本对的损失\n",
    "        loss = 0\n",
    "        for i in range(len(text_features)):\n",
    "            # 获取除了当前正样本对之外的所有负样本对相似度\n",
    "            negative_similarities = torch.cat([similarities[i, :i], similarities[i, i+1:]])\n",
    "            \n",
    "            # 对于每一个负样本对，计算损失\n",
    "            loss += F.relu(self.margin - positive_similarities[i] + negative_similarities).mean()\n",
    "\n",
    "        return loss / len(text_features)\n",
    "\n",
    "# 假设特征维度为 128\n",
    "text_features = torch.randn(3, 128)  # 10 个文本特征\n",
    "image_features = torch.randn(3, 128) # 10 个对应的图像特征\n",
    "\n",
    "# 实例化损失函数\n",
    "contrastive_loss = ContrastiveLoss(margin=0.5)\n",
    "\n",
    "# 计算损失\n",
    "loss = contrastive_loss(text_features, image_features)\n",
    "\n",
    "print(\"Contrastive Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3119, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def sparse_ot(weights1, weights2, M):\n",
    "    \"\"\"Compute Wasserstein distances for one-dimensional weight vectors using PyTorch.\"\"\"\n",
    "    # 确保所有张量都在相同的设备上\n",
    "    device = weights1.device\n",
    "    weights2 = weights2.to(device)\n",
    "    M = M.to(device)\n",
    "    weights1 = F.softplus(weights1)\n",
    "    weights2 = F.softplus(weights2)\n",
    "    # 归一化权重\n",
    "    weights1 = (weights1 + 1e-8) / weights1.sum()\n",
    "    weights2 = (weights2 + 1e-8) / weights2.sum()\n",
    "\n",
    "    # 找到非零元素的索引\n",
    "    active1 = weights1.nonzero(as_tuple=False).squeeze()\n",
    "    active2 = weights2.nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "    # 选择激活的权重和成本矩阵\n",
    "    weights_1_active = weights1[active1]\n",
    "    weights_2_active = weights2[active2]\n",
    "    M_reduced = M[active1][:, active2]\n",
    "\n",
    "    # 计算 Wasserstein 距离\n",
    "    dist = sinkhorn_distance(weights_1_active, weights_2_active, M_reduced)\n",
    "\n",
    "    return dist\n",
    "\n",
    "def sinkhorn_distance(weights1, weights2, M, epsilon=0.01, max_iter=100):\n",
    "    \"\"\"简化的 Sinkhorn 算法实现.\"\"\"\n",
    "    K = torch.exp(-M / epsilon)\n",
    "    u = torch.ones_like(weights1) / weights1.size(0)\n",
    "    for _ in range(max_iter):\n",
    "        u = weights1 / torch.mv(K, weights2 / torch.mv(K.t(), u))\n",
    "    v = weights2 / torch.mv(K.t(), u)\n",
    "    return torch.sum(u * torch.mv(K * M, v))\n",
    "\n",
    "# def sinkhorn_distance(weights1, weights2, M, epsilon=0.01, max_iter=100):\n",
    "#     \"\"\"使用 log-sum-exp 技巧改进的 Sinkhorn 算法实现.\"\"\"\n",
    "#     log_weights1 = torch.log(weights1 + 1e-8)\n",
    "#     log_weights2 = torch.log(weights2 + 1e-8)\n",
    "\n",
    "#     # 计算 cost 矩阵的负指数除以 epsilon\n",
    "#     K = torch.exp(-M / epsilon)\n",
    "#     log_K = torch.log(K + 1e-8)\n",
    "\n",
    "#     u = torch.zeros_like(weights1)\n",
    "#     v = torch.zeros_like(weights2)\n",
    "\n",
    "#     for _ in range(max_iter):\n",
    "#         u = log_weights1 - torch.logsumexp(log_K + v.unsqueeze(0), dim=1)\n",
    "#         v = log_weights2 - torch.logsumexp(log_K.t() + u.unsqueeze(0), dim=1)\n",
    "\n",
    "#     dist = torch.sum(torch.exp(u + v) * M)\n",
    "#     return dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "weights1 = torch.randn(512, device='cuda')\n",
    "\n",
    "weights2 = torch.randn(512, device='cuda')\n",
    "M = torch.ones(512, 512) - torch.eye(512)\n",
    "distances = sparse_ot(weights1, weights2, M)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3119, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(341.9043)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def sinkhorn_distance(weights1, weights2, M, epsilon=0.01, max_iter=100):\n",
    "    \"\"\"使用 Sinkhorn 算法计算两个权重向量之间的 Wasserstein 距离.\"\"\"\n",
    "    # 归一化权重以避免数值问题\n",
    "    weights1 = (weights1 + 1e-8) / weights1.sum()\n",
    "    weights2 = (weights2 + 1e-8) / weights2.sum()\n",
    "\n",
    "    # 确定激活的权重，避免无效计算\n",
    "    active1 = weights1.nonzero(as_tuple=False).squeeze()\n",
    "    active2 = weights2.nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "    # 只对非零元素进行操作\n",
    "    weights_1_active = weights1[active1]\n",
    "    weights_2_active = weights2[active2]\n",
    "    M_reduced = M[active1][:, active2]\n",
    "\n",
    "    # 使用 log-sum-exp 技巧增强数值稳定性\n",
    "    log_weights1 = torch.log(weights_1_active)\n",
    "    log_weights2 = torch.log(weights_2_active)\n",
    "    K = torch.exp(-M_reduced / epsilon)\n",
    "    log_K = torch.log(K + 1e-8)\n",
    "\n",
    "    u = torch.zeros_like(weights_1_active)\n",
    "    v = torch.zeros_like(weights_2_active)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        u = log_weights1 - torch.logsumexp(log_K + v.unsqueeze(0), dim=1)\n",
    "        v = log_weights2 - torch.logsumexp(log_K.t() + u.unsqueeze(0), dim=1)\n",
    "\n",
    "    dist = torch.sum(torch.exp(u + v) * M_reduced)\n",
    "    return dist\n",
    "\n",
    "# 示例使用：\n",
    "weights1 = torch.rand(512,)\n",
    "weights2 = torch.rand(512)\n",
    "M = torch.ones(512, 512) - torch.eye(512)\n",
    "\n",
    "distance = sinkhorn_distance(weights1, weights2, M)\n",
    "print(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2883, 0.2273, 0.2274,  ..., 0.3039, 0.2610, 0.3234],\n",
      "        [0.2725, 0.2882, 0.2596,  ..., 0.2329, 0.2749, 0.2915]])\n",
      "tensor([[0.4182, 0.6278, 0.3556,  ..., 0.4370, 0.5153, 0.4734],\n",
      "        [0.5615, 0.5660, 0.4172,  ..., 0.4120, 0.4559, 0.4857]])\n"
     ]
    }
   ],
   "source": [
    "weights2 = torch.rand(2,30,512)\n",
    "print(torch.std(weights2, dim=1))\n",
    "weights2 = torch.mean(weights2,dim=1)\n",
    "print(weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设的词汇表大小\n",
    "vocab_size = 5  # 包括<pad>, <sos>, <eos>, hello, world\n",
    "\n",
    "# 真实摘要\n",
    "summaries = torch.tensor([[1, 3, 2], [1, 4, 2]])  # <sos>, hello, <eos>\n",
    "\n",
    "# 模型输出（假设的概率分布，通常由模型根据输入文本计算得出）\n",
    "# 这里我们随机生成一些概率分布作为演示\n",
    "outputs = torch.rand(2, 3, vocab_size)  # 3个时间步，每个时间步有一个概率分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [2, 5], got [2, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummaries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [2, 5], got [2, 3]"
     ]
    }
   ],
   "source": [
    "# 交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(outputs, summaries)\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(2, 0))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.empty(0),torch.empty(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(512).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均长度为: 497.7626459143969\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_average_length(directory):\n",
    "    total_length = 0\n",
    "    file_count = 0\n",
    "\n",
    "    # 遍历目录下的所有文件\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):  # 确保处理的是json文件\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                # 提取description字段的内容\n",
    "                if 'description' in data:\n",
    "                    description = data['description']\n",
    "                    total_length += len(description)  # 累加description的长度\n",
    "                    file_count += 1  # 文件计数\n",
    "\n",
    "    # 计算平均长度\n",
    "    if file_count > 0:\n",
    "        average_length = total_length / file_count\n",
    "        return average_length\n",
    "    else:\n",
    "        return 0  # 如果没有适当的json文件或字段，返回0\n",
    "\n",
    "# 使用函数\n",
    "directory = 'test'  # 替换为你的json文件目录\n",
    "average_length = calculate_average_length(directory)\n",
    "print(f\"平均长度为: {average_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "n_points = 3\n",
    "a = np.array([[i, 0] for i in range(n_points)])\n",
    "b = np.array([[i, 1] for i in range(n_points)])\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.scatter(a[:, 0], a[:, 1], label='supp($p(x)$)')\n",
    "plt.scatter(b[:, 0], b[:, 1], label='supp($q(x)$)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinkhorn distance: 1.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from layers import SinkhornDistance\n",
    "\n",
    "x = torch.tensor(a, dtype=torch.float)\n",
    "y = torch.tensor(b, dtype=torch.float)\n",
    "\n",
    "sinkhorn = SinkhornDistance(eps=0.1, max_iter=100, reduction=None)\n",
    "dist, P, C = sinkhorn(x, y)\n",
    "print(\"Sinkhorn distance: {:.3f}\".format(dist.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m y\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m sinkhorn \u001b[38;5;241m=\u001b[39m SinkhornDistance(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m dist, P, C \u001b[38;5;241m=\u001b[39m \u001b[43msinkhorn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSinkhorn distance: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dist\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[0;32m     15\u001b[0m dist\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\assignment2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Study\\Master Document\\Research\\Research Pathway\\code\\layers.py:56\u001b[0m, in \u001b[0;36mSinkhornDistance.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m     55\u001b[0m     u1 \u001b[38;5;241m=\u001b[39m u  \u001b[38;5;66;03m# useful to check the update\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39mlog(mu\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogsumexp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m u\n\u001b[0;32m     57\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39mlog(nu\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-8\u001b[39m) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogsumexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM(C, u, v)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m v\n\u001b[0;32m     58\u001b[0m     err \u001b[38;5;241m=\u001b[39m (u \u001b[38;5;241m-\u001b[39m u1)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32md:\\Study\\Master Document\\Research\\Research Pathway\\code\\layers.py:80\u001b[0m, in \u001b[0;36mSinkhornDistance.M\u001b[1;34m(self, C, u, v)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModified cost for logarithmic updates\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$M_\u001b[39m\u001b[38;5;132;01m{ij}\u001b[39;00m\u001b[38;5;124m = (-c_\u001b[39m\u001b[38;5;132;01m{ij}\u001b[39;00m\u001b[38;5;124m + u_i + v_j) / \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mepsilon$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m v\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from layers import SinkhornDistance\n",
    "\n",
    "x = torch.rand(512, requires_grad=True)\n",
    "y = torch.rand(512, requires_grad=True)\n",
    "\n",
    "x=x.reshape(2,256).to('cuda')\n",
    "y=y.reshape(2,256).to('cuda')\n",
    "\n",
    "\n",
    "sinkhorn = SinkhornDistance(eps=0.1, max_iter=100, reduction=None).to('cuda')\n",
    "\n",
    "dist, P, C = sinkhorn(x, y)\n",
    "print(\"Sinkhorn distance: {:.3f}\".format(dist.item()))\n",
    "dist.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinkhorn distance: 42.606\n",
      "tensor(42.6060, device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from layers import SinkhornDistance\n",
    "\n",
    "x = torch.rand(512, requires_grad=True)\n",
    "y = torch.rand(512, requires_grad=True)\n",
    "\n",
    "x=x.reshape(2,256).to('cuda')\n",
    "y=y.reshape(2,256).to('cuda')\n",
    "\n",
    "\n",
    "sinkhorn = SinkhornDistance(eps=0.1, max_iter=100, reduction=None).to('cuda')\n",
    "\n",
    "dist, P, C = sinkhorn(x, y)\n",
    "print(\"Sinkhorn distance: {:.3f}\".format(dist.item()))\n",
    "# dist.backward()\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1593, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "test_criterion = nn.MSELoss()\n",
    "\n",
    "test_criterion(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liu Sicheng\\AppData\\Local\\Temp\\ipykernel_62096\\217097991.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(dist.grad)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
